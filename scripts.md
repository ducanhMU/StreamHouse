# End-to-End Data Pipeline

## 1. High-level Architecture

```
Data Generator (Python)
        ‚Üì
PostgreSQL (Source)
        ‚Üì  CDC (Debezium)
Kafka Topics
        ‚Üì
Spark Ingest Job
        ‚Üì
HDFS (Delta Lake) + Hive Metastore
        ‚Üì
Spark Process Job
        ‚Üì
PostgreSQL (Destination)

+ OpenMetadata for lineage, metadata & observability
```

This document serves **three purposes**:

1. Explain **what each service does** in the pipeline
2. Standardize **Docker service name / container name / volume name**
3. Provide **step-by-step health checks** to verify the pipeline works end‚Äëto‚Äëend

---

## 2. Service Inventory (Docker Compose)

| #  | Layer         | Service                      | Container                    | Volume(s)                 | Purpose                                            |
| -- | ------------- | ---------------------------- | ---------------------------- | ------------------------- | -------------------------------------------------- |
| 1  | Source        | `data-generator`             | `dp_data_generator`          | ‚Äì                         | Generate mock data and insert into Postgres Source |
| 2  | Source        | `postgres-source`            | `dp_postgres_source`         | `postgres_source_data`    | OLTP source database                               |
| 3  | Destination   | `postgres-dest`              | `dp_postgres_dest`           | `postgres_dest_data`      | Final serving database                             |
| 4  | CDC           | `debezium`                   | `dp_debezium`                | ‚Äì                         | Capture Postgres changes                           |
| 5  | Messaging     | `kafka`                      | `dp_kafka`                   | `kafka_data`              | Event streaming backbone                           |
| 6  | UI            | `kafka-ui`                   | `dp_kafka_ui`                | ‚Äì                         | Inspect Kafka topics & messages                    |
| 7  | Compute       | `spark-master`               | `dp_spark_master`            | ‚Äì                         | Spark cluster master                               |
| 8  | Compute       | `spark-worker`               | `dp_spark_worker`            | ‚Äì                         | Spark worker node                                  |
| 9  | Compute       | `spark-job-ingest`           | `dp_spark_ingest`            | ‚Äì                         | Kafka ‚Üí Delta Lake ingestion                       |
| 10 | Compute       | `spark-job-process`          | `dp_spark_process`           | ‚Äì                         | Delta Lake ‚Üí Postgres Dest                         |
| 11 | Metadata      | `hms-db`                     | `dp_hms_db`                  | `hms_data`                | Hive Metastore database                            |
| 12 | Metadata      | `hms`                        | `hms`                        | ‚Äì                         | Hive Metastore service                             |
| 13 | Storage       | `hdfs-namenode`              | `dp_hdfs_namenode`           | `namenode_data`           | HDFS namespace                                     |
| 14 | Storage       | `hdfs-datanode`              | `dp_hdfs_datanode`           | `datanode_data`           | HDFS data blocks                                   |
| 15 | Resource      | `hdfs-resourcemanager`       | `dp_hdfs_resourcemanager`    | ‚Äì                         | YARN RM                                            |
| 16 | Resource      | `hdfs-nodemanager`           | `dp_hdfs_nodemanager`        | ‚Äì                         | YARN NM                                            |
| 17 | Observability | `openmetadata-mysql`         | `openmetadata_mysql`         | `openmetadata-mysql-data` | OpenMetadata DB                                    |
| 18 | Observability | `openmetadata-elasticsearch` | `openmetadata_elasticsearch` | `es-data`                 | Metadata search                                    |
| 19 | Observability | `execute-migrate-all`        | `execute_migrate_all`        | ‚Äì                         | OpenMetadata schema migration                      |
| 20 | Observability | `openmetadata-server`        | `openmetadata_server`        | ‚Äì                         | OpenMetadata API/UI                                |
| 21 | Observability | `openmetadata-ingestion`     | `openmetadata_ingestion`     | Airflow volumes           | Metadata ingestion & lineage                       |

---

## 3. Health Check Playbook

Follow **top ‚Üí bottom**, matching real data flow.

---

## 3.1 Source Data Layer

### 1Ô∏è‚É£ Data Generator

**Goal:** ensure data is continuously produced

```bash
# Check logs to ensure data is being generated and inserted into Postgres
docker logs -f dp_data_generator
```

‚úÖ Expect logs like:

```
Inserted data into table regions
Inserted data into table customers
```

---

### 2Ô∏è‚É£ Postgres Source

**Goal:** confirm tables & row counts

```bash
# Enter container and check if tables exist and have records
docker exec -it dp_postgres_source psql -U admin -d jadc2_db -c "\dt"

docker exec -it dp_postgres_source psql -U admin -d jadc2_db \
  -c "SELECT count(*) FROM regions;"
# (Replace 'regions' with whatever table your generator creates)
```

‚úÖ Tables exist and row count increases over time

---

## 3.2 Ingestion Layer (CDC)

### 3Ô∏è‚É£ Kafka

**Goal:** Debezium topics created

```bash
# Check if Debezium has created topics for the captured tables
docker exec -it dp_kafka \
  /opt/kafka/bin/kafka-topics.sh \
  --bootstrap-server localhost:9092 --list
```

‚úÖ Expected:

```bash
# You should see topics like:
jadc2_db.public.regions
jadc2_db.public.customers
```

---

### 4Ô∏è‚É£ Debezium

**Goal:** Connector running without errors

```bash
# Check the status of the connectors to ensure they are RUNNING
docker exec -it dp_debezium curl -s http://localhost:8083/connectors

docker exec -it dp_debezium curl -s \
  http://localhost:8083/connectors/postgres-connector/status
```

‚úÖ Expect:

```bash
# Expected output: "state": "RUNNING" for both connector and tasks
"state": "RUNNING"
```

---

### 5Ô∏è‚É£ Kafka UI

**Goal:** Inspect messages visually

```bash
# Check logs to ensure it connected to the Kafka cluster
docker logs dp_kafka_ui
```

‚úÖ UI reachable, topics visible, messages flowing

---

## 3.3 Storage & Metadata Layer

### 6Ô∏è‚É£ HDFS Namenode

**Goal:** HDFS healthy & Delta data exists

```bash
# Check if HDFS is healthy and check for Delta Lake data written by Spark
docker exec -it dp_hdfs_namenode hdfs dfsadmin -report

docker exec -it dp_hdfs_namenode hdfs dfs -ls -R /data/delta
```

‚úÖ Example path:

```bash
# Should show files in /data/delta/silver/regions/...
/data/delta/silver/regions/
```

---

### 7Ô∏è‚É£ Hive Metastore DB

**Goal:** Hive schema initialized

```bash
# Check if Hive has initialized its schema (DBS, TBLS tables should exist)
docker exec -it dp_hms_db psql -U hive -d metastore -c "\dt"

docker exec -it dp_hms_db psql -U hive -d metastore \
  -c "SELECT * FROM \"TBLS\";"
```

‚úÖ Tables like `DBS`, `TBLS`, `SDS` exist

---

### 8Ô∏è‚É£ Hive Metastore Service

```bash
# Check logs to ensure it connected to Postgres and HDFS
docker logs hms
```

‚úÖ Connected to Postgres & HDFS

---

## 3.4 Processing Layer (Spark)

### 9Ô∏è‚É£ Spark Master

```bash
# Check logs to see registered workers and submitted applications
docker logs dp_spark_master
```

‚úÖ Worker registered & applications submitted

---

### üîü Spark Worker

```bash
# Check logs to see if it is executing tasks
docker logs dp_spark_worker
```

‚úÖ Tasks executing

---

### 1Ô∏è‚É£1Ô∏è‚É£ Spark Ingest Job (Kafka ‚Üí Delta)

```bash
# This container runs once (or loops). Check logs for "Success" or Stack Traces.
docker logs -f dp_spark_ingest
```

‚úÖ Look for:

```
INFO DAGScheduler: Job X finished
```

---

### 1Ô∏è‚É£2Ô∏è‚É£ Spark Process Job (Delta ‚Üí Postgres)

```bash
# Check logs. This job waits for HDFS data then writes to Postgres Dest.
docker logs -f dp_spark_process
```

‚úÖ No stack traces, rows written to destination

---

## 3.5 Destination Layer

### 1Ô∏è‚É£3Ô∏è‚É£ Postgres Destination

```bash
# Check if the aggregated/processed tables have been created and populated
docker exec -it dp_postgres_dest psql -U admin -d jadc2_db -c "\\dt"

docker exec -it dp_postgres_dest psql -U admin -d jadc2_db \
  -c "SELECT * FROM processed_data LIMIT 10;"
# (Replace 'processed_data' with your actual destination table name)
```

‚úÖ Aggregated / curated data available

---

## 3.6 Observability (OpenMetadata)

### 1Ô∏è‚É£4Ô∏è‚É£ OpenMetadata MySQL

```bash
# Check connection and schema existence
docker exec -it openmetadata_mysql mysql -uroot -ppassword -e "SHOW DATABASES;"
```

---

### 1Ô∏è‚É£5Ô∏è‚É£ OpenMetadata Elasticsearch

```bash
# Check cluster health
docker exec -it openmetadata_elasticsearch \
  curl -s http://localhost:9200/_cluster/health?pretty
```

‚úÖ `status: green | yellow`

---

### 1Ô∏è‚É£6Ô∏è‚É£ OpenMetadata Migration

```bash
# Check if migration finished successfully (Exit Code 0)
docker logs execute_migrate_all
```

‚úÖ Exit code `0`

---

### 1Ô∏è‚É£7Ô∏è‚É£ OpenMetadata Server

```bash
# Check if the API is up and healthy
docker exec -it openmetadata_server \
  curl -s http://localhost:8585/healthcheck
```

‚úÖ Response:

```json
{"status":"UP"}
```

---

### 1Ô∏è‚É£8Ô∏è‚É£ OpenMetadata Ingestion (Airflow)

```bash
# Check if Airflow scheduler/webserver is running
docker exec -it openmetadata_ingestion airflow dags list

docker logs openmetadata_ingestion
```

‚úÖ DAGs visible, lineage ingestion successful

---

## 4. Mental Model (Interview‚ÄëReady)

* **Postgres** ‚Üí system of record
* **Debezium** ‚Üí change capture
* **Kafka** ‚Üí event log
* **Spark Ingest** ‚Üí bronze/silver Delta
* **Hive Metastore** ‚Üí schema governance
* **Spark Process** ‚Üí business logic
* **Postgres Dest** ‚Üí serving layer
* **OpenMetadata** ‚Üí lineage & trust

## 5. Docker Volumes
```bash
# 1. T·∫°o th∆∞ m·ª•c cha ch·ª©a to√†n b·ªô volume
sudo mkdir -p /drive1/docker_volumes

# 2. T·∫°o c√°c th∆∞ m·ª•c con t∆∞∆°ng ·ª©ng v·ªõi danh s√°ch volume
sudo mkdir -p /drive1/docker_volumes/postgres_source_data
sudo mkdir -p /drive1/docker_volumes/postgres_dest_data
sudo mkdir -p /drive1/docker_volumes/kafka_data
sudo mkdir -p /drive1/docker_volumes/namenode_data
sudo mkdir -p /drive1/docker_volumes/datanode_data
sudo mkdir -p /drive1/docker_volumes/hms_data
sudo mkdir -p /drive1/docker_volumes/ingestion-volume-dag-airflow
sudo mkdir -p /drive1/docker_volumes/ingestion-volume-dags
sudo mkdir -p /drive1/docker_volumes/ingestion-volume-tmp
sudo mkdir -p /drive1/docker_volumes/es-data
sudo mkdir -p /drive1/docker_volumes/openmetadata-mysql-data

# 3. C·∫•p quy·ªÅn ghi t·ªëi ƒëa (777) cho to√†n b·ªô th∆∞ m·ª•c n√†y 
# (B·∫Øt bu·ªôc, n·∫øu kh√¥ng Postgres/ES s·∫Ω crash v√¨ kh√¥ng c√≥ quy·ªÅn ghi)
sudo chmod -R 777 /drive1/docker_volumes
```