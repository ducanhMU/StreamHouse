version: "3.8"

services:
  # ============================================================================
  # SOURCE & DESTINATION DATABASES
  # ============================================================================
  postgres-source:
    container_name: dp_source
    image: postgres:14
    environment:
      POSTGRES_DB: jadc2_db
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: password
      POSTGRES_INITDB_ARGS: --data-checksums
      POSTGRES_HOST_AUTH_METHOD: trust
    command:
      - "postgres"
      - "-c"
      - "wal_level=logical"
      - "-c"
      - "max_wal_senders=10"
      - "-c"
      - "max_replication_slots=10"
      - "-c"
      - "wal_sender_timeout=0"
      - "-c"
      - "max_connections=200"
    ports:
      - "5433:5432"
    volumes:
      - postgres_source_data:/var/lib/postgresql/data
      - ./postgresql/init_source.sql:/docker-entrypoint-initdb.d/init.sql
      - ./postgresql/create_publication.sql:/docker-entrypoint-initdb.d/create_publication.sql
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U admin -d jadc2_db"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - jadc2

  postgres-dest:
    container_name: dp_dest
    image: postgres:14
    environment:
      POSTGRES_DB: jadc2_db
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: password
      POSTGRES_INITDB_ARGS: --data-checksums
      POSTGRES_HOST_AUTH_METHOD: trust
    command:
      - "postgres"
      - "-c"
      - "max_connections=200"
    ports:
      - "5434:5432"
    volumes:
      - postgres_dest_data:/var/lib/postgresql/data
      - ./postgresql/init_dest.sql:/docker-entrypoint-initdb.d/init.sql
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U admin -d jadc2_db"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - jadc2

  # ============================================================================
  # KAFKA (KRaft mode - no Zookeeper needed)
  # ============================================================================
  kafka:
    container_name: dp_kafka
    image: apache/kafka:latest
    ports:
      - "9092:9092"
      - "29092:29092"
    environment:
      # KRaft mode required settings:
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      # define listeners (internal controller + external broker)
      KAFKA_LISTENERS: CONTROLLER://:9093,PLAINTEXT://:9092,PLAINTEXT_HOST://0.0.0.0:29092
      # advertise to clients (outside docker) and internal network
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      # single-node cluster: single controller quorum voter
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka:9093
      # internal topics replication (must be 1 if only one broker)
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      # keep your existing settings if still needed
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
    healthcheck:
      test: nc -z dp_kafka 9092 || exit 1
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 60s
    volumes:
      - kafka_data:/var/lib/kafka/data
    networks:
      - jadc2

  # ============================================================================
  # DEBEZIUM CDC
  # ============================================================================
  debezium:
    container_name: dp_debezium
    image: debezium/connect:2.3
    depends_on:
      kafka:
        condition: service_healthy
      postgres-source:
        condition: service_healthy
    ports:
      - "8083:8083"
    environment:
      BOOTSTRAP_SERVERS: kafka:9092
      GROUP_ID: "1"
      CONFIG_STORAGE_TOPIC: connect_configs
      OFFSET_STORAGE_TOPIC: connect_offsets
      STATUS_STORAGE_TOPIC: connect_statuses
      KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      KEY_CONVERTER_SCHEMAS_ENABLE: "false"
      VALUE_CONVERTER_SCHEMAS_ENABLE: "false"
      OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONFIG_STORAGE_REPLICATION_FACTOR: 1
      STATUS_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_REST_PORT: 8083
      CONNECT_REST_ADVERTISED_HOST_NAME: debezium
      TASKS_MAX: 1
      ENABLE_DEBEZIUM_SCRIPTING: "true"
      CONNECT_LOG4J_ROOT_LOGLEVEL: INFO
    volumes:
      - ./debezium/register-postgres.json:/kafka/config/register-postgres.json:ro
    command: >
      bash -c '
        echo "Waiting for Kafka..."
        cub kafka-ready -b kafka:9092 1 30

        echo "Starting Kafka Connect..."
        /docker-entrypoint.sh start &

        echo "Waiting for Connect to start..."
        until curl -s -o /dev/null -w "%{http_code}" http://localhost:8083/connectors | grep -q 200; do
          sleep 2
        done

        echo "Registering connector..."
        curl -i -X POST -H "Accept:application/json" -H "Content-Type:application/json" http://localhost:8083/connectors/ -d @/kafka/config/register-postgres.json || true

        wait
      '
    healthcheck:
      test: curl -f http://localhost:8083/ || exit 1
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - jadc2

  # ============================================================================
  # KAFKA UI
  # ============================================================================
  kafka-ui:
    container_name: dp_kafka_ui
    image: provectuslabs/kafka-ui:latest
    depends_on:
      kafka:
        condition: service_healthy
    ports:
      - "8085:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:9092
    healthcheck:
      test: curl -f http://localhost:8080 || exit 1
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - jadc2

  # ============================================================================
  # DATA GENERATOR
  # ============================================================================
  data-generator:
    container_name: dp_data_generator
    build:
      context: ./data_generator
      dockerfile: Dockerfile
    environment:
      POSTGRES_DB: jadc2_db
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: password
      POSTGRES_HOST: postgres-source
      POSTGRES_PORT: 5432
      CONTINUOUS_RUN: "true"
      LOOP_INTERVAL_SECONDS: 300
    depends_on:
      postgres-source:
        condition: service_healthy
    networks:
      - jadc2

  # ============================================================================
  # HDFS CLUSTER
  # ============================================================================
  hdfs-namenode:
    container_name: dp_hdfs_namenode
    image: apache/hadoop:3
    hostname: namenode
    command: ["hdfs", "namenode"]
    ports:
      - "9870:9870"
      - "9000:9000"
    env_file:
      - ./config
    environment:
      ENSURE_NAMENODE_DIR: "/tmp/hadoop-root/dfs/name"
    volumes:
      - namenode_data:/tmp/hadoop-root/dfs/name
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9870 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    networks:
      - jadc2

  hdfs-datanode:
    container_name: dp_hdfs_datanode
    image: apache/hadoop:3
    command: ["hdfs", "datanode"]
    env_file:
      - ./config
    volumes:
      - datanode_data:/tmp/hadoop-root/dfs/data
    depends_on:
      hdfs-namenode:
        condition: service_healthy
    networks:
      - jadc2

  hdfs-resourcemanager:
    container_name: dp_hdfs_resourcemanager
    image: apache/hadoop:3
    hostname: resourcemanager
    command: ["yarn", "resourcemanager"]
    ports:
      - "8088:8088"
    env_file:
      - ./config
    volumes:
      - ./test.sh:/opt/test.sh
    depends_on:
      hdfs-namenode:
        condition: service_healthy
    networks:
      - jadc2

  hdfs-nodemanager:
    container_name: dp_hdfs_nodemanager
    image: apache/hadoop:3
    command: ["yarn", "nodemanager"]
    env_file:
      - ./config
    depends_on:
      hdfs-namenode:
        condition: service_healthy
    networks:
      - jadc2

  # ============================================================================
  # HIVE METASTORE
  # ============================================================================
  hms-db:
    container_name: dp_hms_db
    image: postgres:14
    environment:
      POSTGRES_DB: hms
      POSTGRES_USER: hive
      POSTGRES_PASSWORD: hive
      POSTGRES_INITDB_ARGS: --data-checksums
      POSTGRES_HOST_AUTH_METHOD: trust
    command: ["postgres", "-c", "wal_level=logical"]
    ports:
      - "5435:5432"
    volumes:
      - hms_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U hive -d hms"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - jadc2

  hms:
    container_name: dp_hive_metastore
    build:
      context: ./hive-metastore
      dockerfile: Dockerfile
    hostname: hms
    depends_on:
      hdfs-namenode:
        condition: service_healthy
      hms-db:
        condition: service_healthy
    ports:
      - "9083:9083"
    environment:
      HMS_LOGLEVEL: INFO
      HIVE_METASTORE_JDBC_URL: "jdbc:postgresql://hms-db:5432/hms"
      HIVE_METASTORE_USER: "hive"
      HIVE_METASTORE_PASSWORD: "hive"
      HIVE_METASTORE_WAREHOUSE_DIR: "hdfs://namenode:9000/user/hive/warehouse"
    command: >
      bash -c "
        /opt/hive-metastore/bin/schematool -dbType postgres -info -userName hive -passWord hive -url jdbc:postgresql://hms-db:5432/hms >/dev/null 2>&1 ||
        /opt/hive-metastore/bin/schematool -dbType postgres -initSchema -userName hive -passWord hive -url jdbc:postgresql://hms-db:5432/hms;
        /opt/hive-metastore/bin/hive --service metastore
      "
    volumes:
      - ./hive-metastore/conf/hive-site.xml:/opt/hive-metastore/conf/hive-site.xml:ro
    healthcheck:
      test: ["CMD-SHELL", "nc -z hms 9083 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - jadc2

  # ============================================================================
  # SPARK CLUSTER
  # ============================================================================
  spark-master:
    container_name: dp_spark_master
    build:
      context: ./spark
      dockerfile: Dockerfile
    hostname: spark-master
    depends_on:
      hdfs-namenode:
        condition: service_healthy
      hms:
        condition: service_healthy
    ports:
      - "7077:7077"
      - "8082:8080"
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_HOST=spark-master
      - SPARK_MASTER_PORT=7077
      - SPARK_RPC_AUTHENTICATION_ENABLED=yes
      - SPARK_RPC_AUTHENTICATION_SECRET=my-secret
      - SPARK_MASTER_WEBUI_PORT=8080
    volumes:
      - ./jobs:/opt/bitnami/spark/jobs:ro
      - ./spark/conf/hive-site.xml:/opt/spark/conf/hive-site.xml:ro
      - ./spark/conf/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf:ro
      - ./spark/scripts:/opt/bitnami/spark/scripts:ro
    networks:
      - jadc2

  spark-worker:  
    container_name: dp_spark_worker
    build:
      context: ./spark
      dockerfile: Dockerfile
    hostname: spark-worker
    depends_on:
      - spark-master
    ports:
      - "8084:8081"
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
      - SPARK_WORKER_HOSTNAME=spark-worker
      - SPARK_WORKER_WEBUI_PORT=8081
      - SPARK_RPC_AUTHENTICATION_ENABLED=yes
      - SPARK_RPC_AUTHENTICATION_SECRET=my-secret
    volumes:
      - ./jobs:/opt/bitnami/spark/jobs:ro
      - ./spark/conf/hive-site.xml:/opt/spark/conf/hive-site.xml:ro
    networks:
      - jadc2

  # ============================================================================
  # SPARK JOBS
  # ============================================================================
  spark-job-ingest:
    container_name: dp_spark_ingest
    build:
      context: ./spark
      dockerfile: Dockerfile
    depends_on:
      spark-master:
        condition: service_started
      kafka:
        condition: service_healthy
      hdfs-namenode:
        condition: service_healthy
      om-server:
        condition: service_healthy
    environment:
      - SPARK_MASTER_URL=spark://spark-master:7077
      - OM_SERVER_URL=http://om-server:8585
    command: >
      bash -c "
      source /opt/bitnami/spark/scripts/get_om_token.sh

      echo 'Waiting for Spark Master...';
      while ! nc -z spark-master 7077; do sleep 2; done;

      echo 'Waiting for Kafka...';
      while ! nc -z kafka 9092; do sleep 2; done;
      
      echo 'Submitting Ingestion Job...';
      /opt/bitnami/spark/bin/spark-submit \
        --master spark://spark-master:7077 \
        --name 'Job1_Ingest_Kafka_To_HDFS' \
        --deploy-mode client \
        --conf spark.openmetadata.server.url=http://om-server:8585 \
        --conf spark.openmetadata.identity.token=\"\$OM_JWT_TOKEN\" \
        /opt/bitnami/spark/jobs/job1_ingest.py
      "
    networks:
      - jadc2

  spark-job-process:
    container_name: dp_spark_process
    build:
      context: ./spark
      dockerfile: Dockerfile
    depends_on:
      spark-master:
        condition: service_started
      hdfs-namenode:
        condition: service_healthy
      postgres-dest:
        condition: service_healthy
      om-server:
        condition: service_healthy
    environment:
      - SPARK_MASTER_URL=spark://spark-master:7077
      - OM_SERVER_URL=http://om-server:8585
    command: >
      bash -c "
      source /opt/bitnami/spark/scripts/get_om_token.sh

      echo 'Waiting for Spark Master...';
      while ! nc -z spark-master 7077; do sleep 2; done;
      
      echo 'Waiting for Postgres Destination...';
      while ! nc -z postgres-dest 5432; do sleep 2; done;

      echo 'Waiting 30s for Ingest Job to initialize Delta Table...';
      sleep 30;

      echo 'Submitting Processing Job...';
      /opt/bitnami/spark/bin/spark-submit \
        --master spark://spark-master:7077 \
        --name 'Job2_Process_HDFS_To_Postgres' \
        --deploy-mode client \
        --conf spark.openmetadata.server.url=http://om-server:8585 \
        --conf spark.openmetadata.identity.token=\"\$OM_JWT_TOKEN\" \
        /opt/bitnami/spark/jobs/job2_process.py
      "
    networks:
      - jadc2

  # ============================================================================
  # OPENMETADATA - DATABASE
  # ============================================================================
  om-mysql:
    container_name: dp_om_mysql
    image: docker.getcollate.io/openmetadata/db:1.10.0-SNAPSHOT
    command: "--sort_buffer_size=10M"
    restart: always
    environment:
      MYSQL_ROOT_PASSWORD: password
    expose:
      - 3306
    ports:
      - "3307:3306"  # Changed to avoid conflict
    volumes:
      - om_mysql_data:/var/lib/mysql
    networks:
      - jadc2
    healthcheck:
      test: mysql --user=root --password=$$MYSQL_ROOT_PASSWORD --silent --execute "use openmetadata_db"
      interval: 15s
      timeout: 10s
      retries: 10

  # ============================================================================
  # OPENMETADATA - ELASTICSEARCH
  # ============================================================================
  om-elasticsearch:
    container_name: dp_om_elasticsearch
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.4
    environment:
      - discovery.type=single-node
      - ES_JAVA_OPTS=-Xms1024m -Xmx1024m
      - xpack.security.enabled=false
    networks:
      - jadc2
    ports:
      - "9200:9200"
      - "9300:9300"
    healthcheck:
      test: "curl -s http://localhost:9200/_cluster/health?pretty | grep status | grep -qE 'green|yellow' || exit 1"
      interval: 15s
      timeout: 10s
      retries: 10
    volumes:
      - om_es_data:/usr/share/elasticsearch/data

  # ============================================================================
  # OPENMETADATA - MIGRATION
  # ============================================================================
  om-migrate:
    container_name: dp_om_migrate
    image: docker.getcollate.io/openmetadata/server:1.10.0-SNAPSHOT
    command: "./bootstrap/openmetadata-ops.sh migrate"
    environment:
      OPENMETADATA_CLUSTER_NAME: ${OPENMETADATA_CLUSTER_NAME:-openmetadata}
      SERVER_PORT: ${SERVER_PORT:-8585}
      SERVER_ADMIN_PORT: ${SERVER_ADMIN_PORT:-8586}
      LOG_LEVEL: ${LOG_LEVEL:-INFO}
      MIGRATION_LIMIT_PARAM: ${MIGRATION_LIMIT_PARAM:-1200}
      
      # Authentication
      AUTHORIZER_CLASS_NAME: ${AUTHORIZER_CLASS_NAME:-org.openmetadata.service.security.DefaultAuthorizer}
      AUTHORIZER_REQUEST_FILTER: ${AUTHORIZER_REQUEST_FILTER:-org.openmetadata.service.security.JwtFilter}
      AUTHORIZER_ADMIN_PRINCIPALS: ${AUTHORIZER_ADMIN_PRINCIPALS:-[admin]}
      AUTHORIZER_ALLOWED_REGISTRATION_DOMAIN: ${AUTHORIZER_ALLOWED_REGISTRATION_DOMAIN:-["all"]}
      AUTHORIZER_INGESTION_PRINCIPALS: ${AUTHORIZER_INGESTION_PRINCIPALS:-[ingestion-bot]}
      AUTHORIZER_PRINCIPAL_DOMAIN: ${AUTHORIZER_PRINCIPAL_DOMAIN:-"open-metadata.org"}
      AUTHORIZER_ENFORCE_PRINCIPAL_DOMAIN: ${AUTHORIZER_ENFORCE_PRINCIPAL_DOMAIN:-false}
      AUTHORIZER_ENABLE_SECURE_SOCKET: ${AUTHORIZER_ENABLE_SECURE_SOCKET:-false}
      AUTHENTICATION_PROVIDER: ${AUTHENTICATION_PROVIDER:-basic}
      AUTHENTICATION_RESPONSE_TYPE: ${AUTHENTICATION_RESPONSE_TYPE:-id_token}
      AUTHENTICATION_PUBLIC_KEYS: ${AUTHENTICATION_PUBLIC_KEYS:-[http://localhost:8585/api/v1/system/config/jwks]}
      AUTHENTICATION_ENABLE_SELF_SIGNUP: ${AUTHENTICATION_ENABLE_SELF_SIGNUP:-true}
      AUTHENTICATION_CLIENT_TYPE: ${AUTHENTICATION_CLIENT_TYPE:-public}
      
      # JWT
      RSA_PUBLIC_KEY_FILE_PATH: ${RSA_PUBLIC_KEY_FILE_PATH:-"./conf/public_key.der"}
      RSA_PRIVATE_KEY_FILE_PATH: ${RSA_PRIVATE_KEY_FILE_PATH:-"./conf/private_key.der"}
      JWT_ISSUER: ${JWT_ISSUER:-"open-metadata.org"}
      JWT_KEY_ID: ${JWT_KEY_ID:-"Gb389a-9f76-gdjs-a92j-0242bk94356"}
      
      # Database
      DB_DRIVER_CLASS: ${DB_DRIVER_CLASS:-com.mysql.cj.jdbc.Driver}
      DB_SCHEME: ${DB_SCHEME:-mysql}
      DB_PARAMS: ${DB_PARAMS:-allowPublicKeyRetrieval=true&useSSL=false&serverTimezone=UTC}
      DB_USER: ${DB_USER:-openmetadata_user}
      DB_USER_PASSWORD: ${DB_USER_PASSWORD:-openmetadata_password}
      DB_HOST: ${DB_HOST:-om-mysql}
      DB_PORT: ${DB_PORT:-3306}
      OM_DATABASE: ${OM_DATABASE:-openmetadata_db}
      
      # Elasticsearch
      ELASTICSEARCH_HOST: ${ELASTICSEARCH_HOST:-om-elasticsearch}
      ELASTICSEARCH_PORT: ${ELASTICSEARCH_PORT:-9200}
      ELASTICSEARCH_SCHEME: ${ELASTICSEARCH_SCHEME:-http}
      SEARCH_TYPE: ${SEARCH_TYPE:-"elasticsearch"}
      
      # Pipeline Service
      PIPELINE_SERVICE_CLIENT_ENDPOINT: ${PIPELINE_SERVICE_CLIENT_ENDPOINT:-http://om-ingestion:8080}
      SERVER_HOST_API_URL: ${SERVER_HOST_API_URL:-http://om-server:8585/api}
      
      OPENMETADATA_HEAP_OPTS: ${OPENMETADATA_HEAP_OPTS:--Xmx1G -Xms1G}
    depends_on:
      om-elasticsearch:
        condition: service_healthy
      om-mysql:
        condition: service_healthy
    networks:
      - jadc2

  # ============================================================================
  # OPENMETADATA - SERVER
  # ============================================================================
  om-server:
    container_name: dp_om_server
    restart: always
    image: docker.getcollate.io/openmetadata/server:1.10.0-SNAPSHOT
    environment:
      OPENMETADATA_CLUSTER_NAME: ${OPENMETADATA_CLUSTER_NAME:-openmetadata}
      SERVER_PORT: ${SERVER_PORT:-8585}
      SERVER_ADMIN_PORT: ${SERVER_ADMIN_PORT:-8586}
      LOG_LEVEL: ${LOG_LEVEL:-INFO}
      
      # Authentication
      AUTHORIZER_CLASS_NAME: ${AUTHORIZER_CLASS_NAME:-org.openmetadata.service.security.DefaultAuthorizer}
      AUTHORIZER_REQUEST_FILTER: ${AUTHORIZER_REQUEST_FILTER:-org.openmetadata.service.security.JwtFilter}
      AUTHORIZER_ADMIN_PRINCIPALS: ${AUTHORIZER_ADMIN_PRINCIPALS:-[admin]}
      AUTHORIZER_ALLOWED_REGISTRATION_DOMAIN: ${AUTHORIZER_ALLOWED_REGISTRATION_DOMAIN:-["all"]}
      AUTHORIZER_INGESTION_PRINCIPALS: ${AUTHORIZER_INGESTION_PRINCIPALS:-[ingestion-bot]}
      AUTHORIZER_PRINCIPAL_DOMAIN: ${AUTHORIZER_PRINCIPAL_DOMAIN:-"open-metadata.org"}
      AUTHORIZER_ENFORCE_PRINCIPAL_DOMAIN: ${AUTHORIZER_ENFORCE_PRINCIPAL_DOMAIN:-false}
      AUTHORIZER_ENABLE_SECURE_SOCKET: ${AUTHORIZER_ENABLE_SECURE_SOCKET:-false}
      AUTHENTICATION_PROVIDER: ${AUTHENTICATION_PROVIDER:-basic}
      AUTHENTICATION_RESPONSE_TYPE: ${AUTHENTICATION_RESPONSE_TYPE:-id_token}
      AUTHENTICATION_PUBLIC_KEYS: ${AUTHENTICATION_PUBLIC_KEYS:-[http://localhost:8585/api/v1/system/config/jwks]}
      AUTHENTICATION_ENABLE_SELF_SIGNUP: ${AUTHENTICATION_ENABLE_SELF_SIGNUP:-true}
      AUTHENTICATION_CLIENT_TYPE: ${AUTHENTICATION_CLIENT_TYPE:-public}
      
      # JWT
      RSA_PUBLIC_KEY_FILE_PATH: ${RSA_PUBLIC_KEY_FILE_PATH:-"./conf/public_key.der"}
      RSA_PRIVATE_KEY_FILE_PATH: ${RSA_PRIVATE_KEY_FILE_PATH:-"./conf/private_key.der"}
      JWT_ISSUER: ${JWT_ISSUER:-"open-metadata.org"}
      JWT_KEY_ID: ${JWT_KEY_ID:-"Gb389a-9f76-gdjs-a92j-0242bk94356"}
      
      # Pipeline Service
      PIPELINE_SERVICE_CLIENT_ENDPOINT: ${PIPELINE_SERVICE_CLIENT_ENDPOINT:-http://om-ingestion:8080}
      PIPELINE_SERVICE_CLIENT_HEALTH_CHECK_INTERVAL: ${PIPELINE_SERVICE_CLIENT_HEALTH_CHECK_INTERVAL:-300}
      SERVER_HOST_API_URL: ${SERVER_HOST_API_URL:-http://om-server:8585/api}
      PIPELINE_SERVICE_CLIENT_ENABLED: ${PIPELINE_SERVICE_CLIENT_ENABLED:-true}
      
      # Database
      DB_DRIVER_CLASS: ${DB_DRIVER_CLASS:-com.mysql.cj.jdbc.Driver}
      DB_SCHEME: ${DB_SCHEME:-mysql}
      DB_PARAMS: ${DB_PARAMS:-allowPublicKeyRetrieval=true&useSSL=false&serverTimezone=UTC}
      DB_USER: ${DB_USER:-openmetadata_user}
      DB_USER_PASSWORD: ${DB_USER_PASSWORD:-openmetadata_password}
      DB_HOST: ${DB_HOST:-om-mysql}
      DB_PORT: ${DB_PORT:-3306}
      OM_DATABASE: ${OM_DATABASE:-openmetadata_db}
      
      # Elasticsearch
      ELASTICSEARCH_HOST: ${ELASTICSEARCH_HOST:-om-elasticsearch}
      ELASTICSEARCH_PORT: ${ELASTICSEARCH_PORT:-9200}
      ELASTICSEARCH_SCHEME: ${ELASTICSEARCH_SCHEME:-http}
      SEARCH_TYPE: ${SEARCH_TYPE:-"elasticsearch"}
      
      OPENMETADATA_HEAP_OPTS: ${OPENMETADATA_HEAP_OPTS:--Xmx1G -Xms1G}
    
    expose:
      - 8585
      - 8586
    ports:
      - "8585:8585"
      - "8586:8586"
    depends_on:
      om-elasticsearch:
        condition: service_healthy
      om-mysql:
        condition: service_healthy
      om-migrate:
        condition: service_completed_successfully
    networks:
      - jadc2
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:8586/healthcheck"]
      interval: 15s
      timeout: 10s
      retries: 10

  # ============================================================================
  # OPENMETADATA - INGESTION (Airflow)
  # ============================================================================
  om-ingestion:
    container_name: dp_om_ingestion
    image: docker.getcollate.io/openmetadata/ingestion:1.10.0-SNAPSHOT
    depends_on:
      om-elasticsearch:
        condition: service_started
      om-mysql:
        condition: service_healthy
      om-server:
        condition: service_healthy
    environment:
      AIRFLOW__API__AUTH_BACKENDS: "airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session"
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__OPENMETADATA_AIRFLOW_APIS__DAG_GENERATED_CONFIGS: "/opt/airflow/dag_generated_configs"
      DB_HOST: ${AIRFLOW_DB_HOST:-om-mysql}
      DB_PORT: ${AIRFLOW_DB_PORT:-3306}
      AIRFLOW_DB: ${AIRFLOW_DB:-airflow_db}
      DB_SCHEME: ${AIRFLOW_DB_SCHEME:-mysql+mysqldb}
      DB_USER: ${AIRFLOW_DB_USER:-airflow_user}
      DB_PASSWORD: ${AIRFLOW_DB_PASSWORD:-airflow_pass}
      DB_PROPERTIES: ${AIRFLOW_DB_PROPERTIES:-}
    entrypoint: /bin/bash
    command:
      - "/opt/airflow/ingestion_dependency.sh"
    expose:
      - 8080
    ports:
      - "8081:8080"  # Changed to avoid conflict
    networks:
      - jadc2
    volumes:
      - om_ingestion_dag_config:/opt/airflow/dag_generated_configs
      - om_ingestion_dags:/opt/airflow/dags
      - om_ingestion_tmp:/tmp

# ============================================================================
# VOLUMES
# ============================================================================
volumes:
  postgres_source_data:
  postgres_dest_data:
  kafka_data:
  namenode_data:
  datanode_data:
  hms_data:
  om_mysql_data:
  om_es_data:
  om_ingestion_dag_config:
  om_ingestion_dags:
  om_ingestion_tmp:

# ============================================================================
# NETWORKS
# ============================================================================
networks:
  jadc2:
    name: jadc2
    driver: bridge