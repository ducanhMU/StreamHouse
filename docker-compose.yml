version: "3.9"
# ============================================================================
# VOLUMES CONFIGURATION (MAPPED TO /drive1)
# ============================================================================
volumes:
  # 1. Postgres Source
  postgres_source_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /drive1/docker_volumes/postgres_source_data

  # 2. Postgres Dest
  postgres_dest_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /drive1/docker_volumes/postgres_dest_data

  # 3. Kafka
  kafka_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /drive1/docker_volumes/kafka_data

  # 4. HDFS Namenode
  namenode_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /drive1/docker_volumes/namenode_data

  # 5. HDFS Datanode
  datanode_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /drive1/docker_volumes/datanode_data

  # 6. Hive Metastore DB
  hms_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /drive1/docker_volumes/hms_data

  # 7. OpenMetadata - Ingestion Airflow Config
  ingestion-volume-dag-airflow:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /drive1/docker_volumes/ingestion-volume-dag-airflow

  # 8. OpenMetadata - Ingestion DAGs
  ingestion-volume-dags:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /drive1/docker_volumes/ingestion-volume-dags

  # 9. OpenMetadata - Ingestion Tmp
  ingestion-volume-tmp:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /drive1/docker_volumes/ingestion-volume-tmp

  # 10. OpenMetadata - Elasticsearch
  es-data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /drive1/docker_volumes/es-data

  # 11. OpenMetadata - MySQL
  openmetadata-mysql-data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /drive1/docker_volumes/openmetadata-mysql-data

# ============================================================================
# NETWORKS
# ============================================================================
networks:
  jadc2:
    name: jadc2
    driver: bridge

# ============================================================================
# SERVICES
# ============================================================================
services:
  # ============================================================================
  # OPENMETADATA - MySQL Database
  # ============================================================================
  openmetadata-mysql:
    container_name: openmetadata_mysql
    image: docker.getcollate.io/openmetadata/db:1.11.4
    command: "--sort_buffer_size=10M"
    restart: always
    environment:
      MYSQL_ROOT_PASSWORD: password
    expose:
      - 3306
    ports:
      - "3307:3306"  # Changed port to avoid conflict
    volumes:
      - openmetadata-mysql-data:/var/lib/mysql
    networks:
      - jadc2
    healthcheck:
      test: mysql --user=root --password=$$MYSQL_ROOT_PASSWORD --silent --execute "use openmetadata_db"
      interval: 15s
      timeout: 10s
      retries: 10

  # ============================================================================
  # OPENMETADATA - Elasticsearch
  # ============================================================================
  openmetadata-elasticsearch:
    container_name: openmetadata_elasticsearch
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.4
    environment:
      - discovery.type=single-node
      - ES_JAVA_OPTS=-Xms1024m -Xmx1024m
      - xpack.security.enabled=false
    networks:
      - jadc2
    ports:
      - "9200:9200"
      - "9300:9300"
    healthcheck:
      test: "curl -s http://localhost:9200/_cluster/health?pretty | grep status | grep -qE 'green|yellow' || exit 1"
      interval: 15s
      timeout: 10s
      retries: 10
    volumes:
      - es-data:/usr/share/elasticsearch/data

  # ============================================================================
  # OPENMETADATA - Migration Service
  # ============================================================================
  execute-migrate-all:
    container_name: execute_migrate_all
    image: docker.getcollate.io/openmetadata/server:1.11.4
    command: "./bootstrap/openmetadata-ops.sh migrate"
    environment:
      OPENMETADATA_CLUSTER_NAME: ${OPENMETADATA_CLUSTER_NAME:-openmetadata}
      SERVER_PORT: ${SERVER_PORT:-8585}
      SERVER_ADMIN_PORT: ${SERVER_ADMIN_PORT:-8586}
      LOG_LEVEL: ${LOG_LEVEL:-INFO}
      
      MIGRATION_LIMIT_PARAM: ${MIGRATION_LIMIT_PARAM:-1200}
      
      AUTHORIZER_CLASS_NAME: ${AUTHORIZER_CLASS_NAME:-org.openmetadata.service.security.DefaultAuthorizer}
      AUTHORIZER_REQUEST_FILTER: ${AUTHORIZER_REQUEST_FILTER:-org.openmetadata.service.security.JwtFilter}
      AUTHORIZER_ADMIN_PRINCIPALS: ${AUTHORIZER_ADMIN_PRINCIPALS:-[admin]}
      AUTHORIZER_INGESTION_PRINCIPALS: ${AUTHORIZER_INGESTION_PRINCIPALS:-[ingestion-bot]}
      AUTHENTICATION_PROVIDER: ${AUTHENTICATION_PROVIDER:-basic}
      AUTHENTICATION_PUBLIC_KEYS: ${AUTHENTICATION_PUBLIC_KEYS:-[http://localhost:8585/api/v1/system/config/jwks]}
      AUTHENTICATION_ENABLE_SELF_SIGNUP: ${AUTHENTICATION_ENABLE_SELF_SIGNUP:-true}
      
      RSA_PUBLIC_KEY_FILE_PATH: ${RSA_PUBLIC_KEY_FILE_PATH:-"./conf/public_key.der"}
      RSA_PRIVATE_KEY_FILE_PATH: ${RSA_PRIVATE_KEY_FILE_PATH:-"./conf/private_key.der"}
      JWT_ISSUER: ${JWT_ISSUER:-"open-metadata.org"}
      JWT_KEY_ID: ${JWT_KEY_ID:-"Gb389a-9f76-gdjs-a92j-0242bk94356"}
      
      PIPELINE_SERVICE_CLIENT_ENDPOINT: ${PIPELINE_SERVICE_CLIENT_ENDPOINT:-http://openmetadata-ingestion:8080}
      SERVER_HOST_API_URL: ${SERVER_HOST_API_URL:-http://openmetadata-server:8585/api}
      
      DB_DRIVER_CLASS: ${DB_DRIVER_CLASS:-com.mysql.cj.jdbc.Driver}
      DB_SCHEME: ${DB_SCHEME:-mysql}
      DB_PARAMS: ${DB_PARAMS:-allowPublicKeyRetrieval=true&useSSL=false&serverTimezone=UTC}
      DB_USER: ${DB_USER:-openmetadata_user}
      DB_USER_PASSWORD: ${DB_USER_PASSWORD:-openmetadata_password}
      DB_HOST: ${DB_HOST:-openmetadata-mysql}
      DB_PORT: ${DB_PORT:-3306}
      OM_DATABASE: ${OM_DATABASE:-openmetadata_db}
      
      ELASTICSEARCH_HOST: ${ELASTICSEARCH_HOST:-openmetadata-elasticsearch}
      ELASTICSEARCH_PORT: ${ELASTICSEARCH_PORT:-9200}
      ELASTICSEARCH_SCHEME: ${ELASTICSEARCH_SCHEME:-http}
      SEARCH_TYPE: ${SEARCH_TYPE:-elasticsearch}
      
      PIPELINE_SERVICE_CLIENT_ENABLED: ${PIPELINE_SERVICE_CLIENT_ENABLED:-true}
      AIRFLOW_USERNAME: ${AIRFLOW_USERNAME:-admin}
      AIRFLOW_PASSWORD: ${AIRFLOW_PASSWORD:-admin}
      FERNET_KEY: ${FERNET_KEY:-jJ/9sz0g0OHxsfxOoSfdFdmk3ysNmPRnH3TUAbz3IHA=}
      
      SECRET_MANAGER: ${SECRET_MANAGER:-db}
      OPENMETADATA_HEAP_OPTS: ${OPENMETADATA_HEAP_OPTS:--Xmx1G -Xms1G}
      
    depends_on:
      openmetadata-elasticsearch:
        condition: service_healthy
      openmetadata-mysql:
        condition: service_healthy
    networks:
      - jadc2

  # ============================================================================
  # OPENMETADATA - Server
  # ============================================================================
  openmetadata-server:
    container_name: openmetadata_server
    restart: always
    image: docker.getcollate.io/openmetadata/server:1.11.4
    environment:
      OPENMETADATA_CLUSTER_NAME: ${OPENMETADATA_CLUSTER_NAME:-openmetadata}
      SERVER_PORT: ${SERVER_PORT:-8585}
      SERVER_ADMIN_PORT: ${SERVER_ADMIN_PORT:-8586}
      LOG_LEVEL: ${LOG_LEVEL:-INFO}
      
      AUTHORIZER_CLASS_NAME: ${AUTHORIZER_CLASS_NAME:-org.openmetadata.service.security.DefaultAuthorizer}
      AUTHORIZER_REQUEST_FILTER: ${AUTHORIZER_REQUEST_FILTER:-org.openmetadata.service.security.JwtFilter}
      AUTHORIZER_ADMIN_PRINCIPALS: ${AUTHORIZER_ADMIN_PRINCIPALS:-[admin]}
      AUTHORIZER_INGESTION_PRINCIPALS: ${AUTHORIZER_INGESTION_PRINCIPALS:-[ingestion-bot]}
      AUTHENTICATION_PROVIDER: ${AUTHENTICATION_PROVIDER:-basic}
      AUTHENTICATION_PUBLIC_KEYS: ${AUTHENTICATION_PUBLIC_KEYS:-[http://localhost:8585/api/v1/system/config/jwks]}
      AUTHENTICATION_ENABLE_SELF_SIGNUP: ${AUTHENTICATION_ENABLE_SELF_SIGNUP:-true}
      
      RSA_PUBLIC_KEY_FILE_PATH: ${RSA_PUBLIC_KEY_FILE_PATH:-"./conf/public_key.der"}
      RSA_PRIVATE_KEY_FILE_PATH: ${RSA_PRIVATE_KEY_FILE_PATH:-"./conf/private_key.der"}
      JWT_ISSUER: ${JWT_ISSUER:-"open-metadata.org"}
      JWT_KEY_ID: ${JWT_KEY_ID:-"Gb389a-9f76-gdjs-a92j-0242bk94356"}
      
      PIPELINE_SERVICE_CLIENT_ENDPOINT: ${PIPELINE_SERVICE_CLIENT_ENDPOINT:-http://openmetadata-ingestion:8080}
      SERVER_HOST_API_URL: ${SERVER_HOST_API_URL:-http://openmetadata-server:8585/api}
      
      DB_DRIVER_CLASS: ${DB_DRIVER_CLASS:-com.mysql.cj.jdbc.Driver}
      DB_SCHEME: ${DB_SCHEME:-mysql}
      DB_PARAMS: ${DB_PARAMS:-allowPublicKeyRetrieval=true&useSSL=false&serverTimezone=UTC}
      DB_USER: ${DB_USER:-openmetadata_user}
      DB_USER_PASSWORD: ${DB_USER_PASSWORD:-openmetadata_password}
      DB_HOST: ${DB_HOST:-openmetadata-mysql}
      DB_PORT: ${DB_PORT:-3306}
      OM_DATABASE: ${OM_DATABASE:-openmetadata_db}
      
      ELASTICSEARCH_HOST: ${ELASTICSEARCH_HOST:-openmetadata-elasticsearch}
      ELASTICSEARCH_PORT: ${ELASTICSEARCH_PORT:-9200}
      ELASTICSEARCH_SCHEME: ${ELASTICSEARCH_SCHEME:-http}
      SEARCH_TYPE: ${SEARCH_TYPE:-elasticsearch}
      
      EVENT_MONITOR: ${EVENT_MONITOR:-prometheus}
      
      PIPELINE_SERVICE_CLIENT_ENABLED: ${PIPELINE_SERVICE_CLIENT_ENABLED:-true}
      AIRFLOW_USERNAME: ${AIRFLOW_USERNAME:-admin}
      AIRFLOW_PASSWORD: ${AIRFLOW_PASSWORD:-admin}
      FERNET_KEY: ${FERNET_KEY:-jJ/9sz0g0OHxsfxOoSfdFdmk3ysNmPRnH3TUAbz3IHA=}
      
      SECRET_MANAGER: ${SECRET_MANAGER:-db}
      OPENMETADATA_HEAP_OPTS: ${OPENMETADATA_HEAP_OPTS:--Xmx1G -Xms1G}
    
    expose:
      - 8585
      - 8586
    ports:
      - "8585:8585"
      - "8586:8586"
    depends_on:
      openmetadata-elasticsearch:
        condition: service_healthy
      openmetadata-mysql:
        condition: service_healthy
      execute-migrate-all:
        condition: service_completed_successfully
    networks:
      - jadc2
    healthcheck:
      test: [ "CMD", "wget", "-q", "--spider",  "http://localhost:8586/healthcheck" ]

  # ============================================================================
  # OPENMETADATA - Ingestion (Airflow)
  # ============================================================================
  openmetadata-ingestion:
    container_name: openmetadata_ingestion
    image: docker.getcollate.io/openmetadata/ingestion:1.11.4
    depends_on:
      openmetadata-elasticsearch:
        condition: service_started
      openmetadata-mysql:
        condition: service_healthy
      openmetadata-server:
        condition: service_started
    environment:
      AIRFLOW__API__AUTH_BACKENDS: "airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session"
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__OPENMETADATA_AIRFLOW_APIS__DAG_GENERATED_CONFIGS: "/opt/airflow/dag_generated_configs"
      DB_HOST: ${AIRFLOW_DB_HOST:-openmetadata-mysql}
      DB_PORT: ${AIRFLOW_DB_PORT:-3306}
      AIRFLOW_DB: ${AIRFLOW_DB:-airflow_db}
      DB_SCHEME: ${AIRFLOW_DB_SCHEME:-mysql+mysqldb}
      DB_USER: ${AIRFLOW_DB_USER:-airflow_user}
      DB_PASSWORD: ${AIRFLOW_DB_PASSWORD:-airflow_pass}
      DB_PROPERTIES: ${AIRFLOW_DB_PROPERTIES:-}
    entrypoint: /bin/bash
    command:
      - "/opt/airflow/ingestion_dependency.sh"
    expose:
      - 8080
    ports:
      - "8087:8080"  # Changed port to avoid conflict
    networks:
      - jadc2
    volumes:
      - ingestion-volume-dag-airflow:/opt/airflow/dag_generated_configs
      - ingestion-volume-dags:/opt/airflow/dags
      - ingestion-volume-tmp:/tmp

  # ============================================================================
  # SOURCE DATABASE (Có cấu hình WAL cho Debezium + Stats cho OpenMetadata)
  # ============================================================================
  postgres-source:
    container_name: dp_postgres_source
    image: postgres:14
    environment:
      POSTGRES_DB: jadc2_db
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: password
      POSTGRES_INITDB_ARGS: --data-checksums
      POSTGRES_HOST_AUTH_METHOD: trust
    command:
      - "postgres"
      - "-c"
      - "wal_level=logical"
      - "-c"
      - "max_wal_senders=10"
      - "-c"
      - "max_replication_slots=10"
      - "-c"
      - "wal_sender_timeout=0"
      - "-c"
      - "max_connections=200"
      - "-c"
      - "shared_preload_libraries=pg_stat_statements"
      - "-c"
      - "pg_stat_statements.track=all"
      - "-c"
      - "pg_stat_statements.max=10000"
      - "-c"
      - "track_activity_query_size=2048"
    ports:
      - "5433:5432"
    volumes:
      - postgres_source_data:/var/lib/postgresql/data
      - ./postgresql/init_source.sql:/docker-entrypoint-initdb.d/init.sql
      - ./postgresql/create_publication.sql:/docker-entrypoint-initdb.d/create_publication.sql
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U admin -d jadc2_db"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - jadc2
    
  # ============================================================================
  # DESTINATION DATABASE (Thêm Stats cho OpenMetadata)
  # ============================================================================
  postgres-dest:
    container_name: dp_postgres_dest
    image: postgres:14
    environment:
      POSTGRES_DB: jadc2_db
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: password
      POSTGRES_INITDB_ARGS: --data-checksums
      POSTGRES_HOST_AUTH_METHOD: trust
    command:
      - "postgres"
      - "-c"
      - "max_connections=200"
      - "-c"
      - "shared_preload_libraries=pg_stat_statements"
      - "-c"
      - "pg_stat_statements.track=all"
      - "-c"
      - "pg_stat_statements.max=10000"
      - "-c"
      - "track_activity_query_size=2048"
    ports:
      - "5434:5432"
    volumes:
      - postgres_dest_data:/var/lib/postgresql/data
      - ./postgresql/init_dest.sql:/docker-entrypoint-initdb.d/init.sql
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U admin -d jadc2_db"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - jadc2

  # ============================================================================
  # DATA GENERATOR
  # ============================================================================
  data-generator:
    container_name: dp_data_generator
    build:
      context: ./data_generator
      dockerfile: Dockerfile
    environment:
      POSTGRES_DB: jadc2_db
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: password
      POSTGRES_HOST: postgres-source
      POSTGRES_PORT: 5432
      CONTINUOUS_RUN: "true"
      LOOP_INTERVAL_SECONDS: 300
    depends_on:
      postgres-source:
        condition: service_healthy
    healthcheck:
      interval: 15s
      timeout: 10s
      retries: 10
    networks:
      - jadc2

  # ============================================================================
  # KAFKA (KRaft mode - no Zookeeper needed)
  # ============================================================================
  kafka:
    container_name: dp_kafka
    image: apache/kafka:latest
    ports:
      - "9092:9092"
      - "29092:29092"
    environment:
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_LISTENERS: CONTROLLER://:9093,PLAINTEXT://:9092,PLAINTEXT_HOST://0.0.0.0:29092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka:9093
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
    healthcheck:
      test: nc -z dp_kafka 9092 || exit 1
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 60s
    volumes:
      - kafka_data:/var/lib/kafka/data
    networks:
      - jadc2

  # ============================================================================
  # DEBEZIUM CDC
  # ============================================================================
  debezium:
    container_name: dp_debezium
    image: debezium/connect:2.3
    depends_on:
      kafka:
        condition: service_healthy
      postgres-source:
        condition: service_healthy
    ports:
      - "8083:8083"
    environment:
      BOOTSTRAP_SERVERS: kafka:9092
      GROUP_ID: "1"
      CONFIG_STORAGE_TOPIC: connect_configs
      OFFSET_STORAGE_TOPIC: connect_offsets
      STATUS_STORAGE_TOPIC: connect_statuses
      KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      KEY_CONVERTER_SCHEMAS_ENABLE: "false"
      VALUE_CONVERTER_SCHEMAS_ENABLE: "false"
      OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONFIG_STORAGE_REPLICATION_FACTOR: 1
      STATUS_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_REST_PORT: 8083
      CONNECT_REST_ADVERTISED_HOST_NAME: debezium
      TASKS_MAX: 1
      ENABLE_DEBEZIUM_SCRIPTING: "true"
      CONNECT_LOG4J_ROOT_LOGLEVEL: INFO
    volumes:
      - ./debezium/register-postgres.json:/kafka/config/register-postgres.json:ro
    command: >
      bash -c '
        echo "Waiting for Kafka..."
        cub kafka-ready -b kafka:9092 1 30

        echo "Starting Kafka Connect..."
        /docker-entrypoint.sh start &

        echo "Waiting for Connect to start..."
        until curl -s -o /dev/null -w "%{http_code}" http://localhost:8083/connectors | grep -q 200; do
          sleep 2
        done

        echo "Registering connector..."
        curl -i -X POST -H "Accept:application/json" -H "Content-Type:application/json" http://localhost:8083/connectors/ -d @/kafka/config/register-postgres.json || true

        wait
      '
    healthcheck:
      test: curl -f http://localhost:8083/ || exit 1
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - jadc2

  # ============================================================================
  # KAFKA UI
  # ============================================================================
  kafka-ui:
    container_name: dp_kafka_ui
    image: provectuslabs/kafka-ui:latest
    depends_on:
      kafka:
        condition: service_healthy
    ports:
      - "8088:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:9092
    healthcheck:
      test: curl -f http://localhost:8080 || exit 1
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - jadc2

  # ============================================================================
  # HDFS CLUSTER
  # ============================================================================
  hdfs-namenode:
    container_name: dp_hdfs_namenode
    image: apache/hadoop:3
    hostname: namenode
    user: "root"
    command: ["/bin/bash", "-c", "chmod -R 777 /tmp/hadoop-root/dfs/name; if [ ! -d /tmp/hadoop-root/dfs/name/current ]; then echo 'Formatting NameNode...'; hdfs namenode -format -force -nonInteractive; fi; hdfs namenode"]
    ports:
      - "9870:9870"
      - "9003:9000"
    environment:
      HDFS_NAMENODE_USER: "root"
      CORE-SITE.XML_fs.defaultFS: "hdfs://namenode:9000"
      HDFS-SITE.XML_dfs.replication: "1"
      HDFS-SITE.XML_dfs.namenode.name.dir: "/tmp/hadoop-root/dfs/name"
    volumes:
      - namenode_data:/tmp/hadoop-root/dfs/name
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9870 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    networks:
      - jadc2

  hdfs-datanode:
    container_name: dp_hdfs_datanode
    image: apache/hadoop:3
    command: ["hdfs", "datanode"]
    user: "root"
    environment:
      HDFS_DATANODE_USER: "root"
      CORE-SITE.XML_fs.defaultFS: "hdfs://namenode:9000"
      HDFS-SITE.XML_dfs.replication: "1"
    volumes:
      - datanode_data:/tmp/hadoop-root/dfs/data
    depends_on:
      hdfs-namenode:
        condition: service_healthy
    networks:
      - jadc2

  hdfs-resourcemanager:
    container_name: dp_hdfs_resourcemanager
    image: apache/hadoop:3
    hostname: resourcemanager
    command: ["yarn", "resourcemanager"]
    user: "root"
    ports:
      - "8089:8088"
    environment:
      YARN_RESOURCEMANAGER_USER: "root"
      CORE-SITE.XML_fs.defaultFS: "hdfs://namenode:9000"
      HDFS-SITE.XML_dfs.replication: "1"
      YARN-SITE.XML_yarn.resourcemanager.hostname: "resourcemanager"
    depends_on:
      hdfs-namenode:
        condition: service_healthy
    networks:
      - jadc2

  hdfs-nodemanager:
    container_name: dp_hdfs_nodemanager
    image: apache/hadoop:3
    command: ["yarn", "nodemanager"]
    user: "root"
    environment:
      YARN_NODEMANAGER_USER: "root"
      CORE-SITE.XML_fs.defaultFS: "hdfs://namenode:9000"
      HDFS-SITE.XML_dfs.replication: "1"
      YARN-SITE.XML_yarn.resourcemanager.hostname: "resourcemanager"
    depends_on:
      hdfs-namenode:
        condition: service_healthy
    networks:
      - jadc2   

  # ============================================================================
  # HIVE METASTORE DATABASE (PostgreSQL)
  # ============================================================================
  hms-db:
    container_name: dp_hms_db
    image: postgres:14
    environment:
      POSTGRES_DB: metastore
      POSTGRES_USER: hive
      POSTGRES_PASSWORD: hive
      POSTGRES_INITDB_ARGS: --data-checksums
      POSTGRES_HOST_AUTH_METHOD: trust
    command: ["postgres", "-c", "wal_level=logical"]
    ports:
      - "5435:5432"
    volumes:
      - hms_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U hive -d metastore"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - jadc2

  # ============================================================================
  # HIVE METASTORE SERVICE
  # ============================================================================
  hms:
    container_name: hms
    build:
      context: ./hive-metastore
      dockerfile: Dockerfile
    hostname: hms
    depends_on:
      hdfs-namenode:
        condition: service_healthy
      hms-db:
        condition: service_healthy
    ports:
      - "9083:9083"
    environment:
      POSTGRES_HOST: hms-db
      POSTGRES_PORT: 5432
      POSTGRES_DB: metastore
      POSTGRES_USER: hive
      POSTGRES_PASSWORD: hive
      HDFS_NAMENODE_HOST: namenode
      HDFS_NAMENODE_PORT: 9000
      METASTORE_PORT: 9083
    volumes:
      - ./hive-metastore/conf/hive-site.xml:/opt/hive/conf/hive-site.xml:ro
    healthcheck:
      test: ["CMD-SHELL", "/opt/healthcheck.sh"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - jadc2

  # ============================================================================
  # SPARK CLUSTER
  # ============================================================================
  spark-master:
    container_name: dp_spark_master
    build:
      context: ./spark
      dockerfile: Dockerfile
    hostname: spark-master
    depends_on:
      hdfs-namenode:
        condition: service_healthy
      hms:
        condition: service_healthy
    ports:
      - "7077:7077"
      - "8082:8080"
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=yes
      - SPARK_RPC_AUTHENTICATION_SECRET=jadc2-secret
      - SPARK_MASTER_HOST=spark-master
    volumes:
      - ./spark/jobs:/opt/spark/jobs:ro
      - ./spark/conf/hive-site.xml:/opt/spark/conf/hive-site.xml:ro
      - ./spark/conf/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf:ro
    command: ["/opt/spark/bin/spark-class", "org.apache.spark.deploy.master.Master"]
    networks:
      - jadc2

  spark-worker:  
    container_name: dp_spark_worker
    build:
      context: ./spark
      dockerfile: Dockerfile
    hostname: spark-worker
    depends_on:
      - spark-master
    ports:
      - "8084:8081"
    environment:
      - SPARK_MODE=worker
      - SPARK_WORKER_CORES=4
      - SPARK_WORKER_MEMORY=4g
      - SPARK_RPC_AUTHENTICATION_ENABLED=yes
      - SPARK_RPC_AUTHENTICATION_SECRET=jadc2-secret
    volumes:
      - ./spark/jobs:/opt/spark/jobs:ro
      - ./spark/conf/hive-site.xml:/opt/spark/conf/hive-site.xml:ro
      - ./spark/conf/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf:ro
    command: ["/opt/spark/bin/spark-class", "org.apache.spark.deploy.worker.Worker", "spark://spark-master:7077"]
    networks:
      - jadc2

  # ============================================================================
  # SPARK JOBS
  # ============================================================================
  spark-job-ingest:
    container_name: dp_spark_ingest
    build:
      context: ./spark
      dockerfile: Dockerfile
    depends_on:
      spark-master:
        condition: service_started
      kafka:
        condition: service_healthy
      hms:
        condition: service_healthy
      openmetadata-server:
        condition: service_started
    environment:
      - SPARK_RPC_AUTHENTICATION_ENABLED=yes
      - SPARK_RPC_AUTHENTICATION_SECRET=jadc2-secret
      - HADOOP_USER_NAME=root
      - OM_SERVER_HOST=openmetadata-server
      - OM_JWT_TOKEN=${OM_JWT_TOKEN}
    volumes:
      - ./spark/scripts:/opt/scripts:ro
      - ./spark/jobs:/opt/spark/jobs:ro
      - ./spark/conf/hive-site.xml:/opt/spark/conf/hive-site.xml:ro
      - ./spark/conf/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf:ro
    command:
      - bash
      - -c
      - |
        echo 'Waiting for Spark Master...'
        while ! nc -z spark-master 7077; do sleep 2; done

        echo 'Waiting for Kafka...'
        while ! nc -z kafka 9092; do sleep 2; done
        
        echo 'Submitting Ingestion Job...'
        /opt/spark/bin/spark-submit \
          --master spark://spark-master:7077 \
          --name 'Job1_Ingest_Kafka_To_HDFS' \
          --deploy-mode client \
          --conf spark.hadoop.hive.metastore.uris=thrift://hms:9083 \
          --conf spark.cores.max=2 \
          --conf spark.executor.memory=1500m \
          --conf spark.extraListeners=io.openlineage.spark.agent.OpenLineageSparkListener \
          --conf spark.openmetadata.transport.hostPort=http://openmetadata-server:8585/api \
          --conf spark.openmetadata.transport.type=openmetadata \
          --conf spark.openmetadata.transport.jwtToken="$OM_JWT_TOKEN" \
          /opt/spark/jobs/job1_demo.py
    networks:
      - jadc2

  spark-job-process:
    container_name: dp_spark_process
    build:
      context: ./spark
      dockerfile: Dockerfile
    depends_on:
      spark-master:
        condition: service_started
      hms:
        condition: service_healthy
      postgres-dest:
        condition: service_healthy
      openmetadata-server:
        condition: service_started
    environment:
      - SPARK_RPC_AUTHENTICATION_ENABLED=yes
      - SPARK_RPC_AUTHENTICATION_SECRET=jadc2-secret
      - HADOOP_USER_NAME=root
      - OM_SERVER_HOST=openmetadata-server
      - OM_JWT_TOKEN=${OM_JWT_TOKEN}
    volumes:
      - ./spark/jobs:/opt/spark/jobs:ro
      - ./spark/conf/hive-site.xml:/opt/spark/conf/hive-site.xml:ro
      - ./spark/conf/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf:ro
      - ./spark/scripts:/opt/spark/scripts:ro
    command:
      - bash
      - -c
      - |
        echo 'Waiting for Spark Master...'
        while ! nc -z spark-master 7077; do sleep 2; done
        
        echo 'Waiting for Postgres Destination...'
        while ! nc -z postgres-dest 5432; do sleep 2; done

        echo 'Checking Data Availability (Silver Layer)...'
        
        until /opt/spark/bin/spark-submit --master local[*] /opt/spark/scripts/check_hdfs_path.py "hdfs://namenode:9000/data/delta/silver/regions/_delta_log"; do
          echo 'Silver data not found yet. Sleeping 20s...'
          sleep 20
        done
        
        echo 'Data Found! Submitting Processing Job...'
        /opt/spark/bin/spark-submit \
          --master spark://spark-master:7077 \
          --name 'Job2_Process_HDFS_To_Postgres' \
          --deploy-mode client \
          --conf spark.hadoop.hive.metastore.uris=thrift://hms:9083 \
          --conf spark.cores.max=2 \
          --conf spark.executor.memory=1500m \
          --conf spark.extraListeners=io.openlineage.spark.agent.OpenLineageSparkListener \
          --conf spark.openmetadata.transport.hostPort=http://openmetadata-server:8585/api \
          --conf spark.openmetadata.transport.type=openmetadata \
          --conf spark.openmetadata.transport.jwtToken="$OM_JWT_TOKEN" \
          /opt/spark/jobs/job2_demo.py
    networks:
      - jadc2