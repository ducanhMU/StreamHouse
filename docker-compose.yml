version: "3.8"
services:
  # ============================================================================
  # SOURCE DATABASE (C√≥ c·∫•u h√¨nh WAL cho Debezium + Stats cho OpenMetadata)
  # ============================================================================
  postgres-source:
    container_name: dp_postgres_source
    image: postgres:14
    environment:
      POSTGRES_DB: jadc2_db
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: password
      POSTGRES_INITDB_ARGS: --data-checksums
      POSTGRES_HOST_AUTH_METHOD: trust
    command:
      - "postgres"
      - "-c"
      - "wal_level=logical"          # C·∫ßn cho Debezium (CDC)
      - "-c"
      - "max_wal_senders=10"
      - "-c"
      - "max_replication_slots=10"
      - "-c"
      - "wal_sender_timeout=0"
      - "-c"
      - "max_connections=200"
      # --- TH√äM PH·∫¶N N√ÄY CHO OPENMETADATA ---
      - "-c"
      - "shared_preload_libraries=pg_stat_statements"
      - "-c"
      - "pg_stat_statements.track=all"
      - "-c"
      - "pg_stat_statements.max=10000"
      - "-c"
      - "track_activity_query_size=2048"
    ports:
      - "5433:5432"
    volumes:
      - postgres_source_data:/var/lib/postgresql/data
      - ./postgresql/init_source.sql:/docker-entrypoint-initdb.d/init.sql
      - ./postgresql/create_publication.sql:/docker-entrypoint-initdb.d/create_publication.sql
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U admin -d jadc2_db"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - jadc2
    
  # ============================================================================
  # DESTINATION DATABASE (Th√™m Stats cho OpenMetadata)
  # ============================================================================
  postgres-dest:
    container_name: dp_postgres_dest
    image: postgres:14
    environment:
      POSTGRES_DB: jadc2_db
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: password
      POSTGRES_INITDB_ARGS: --data-checksums
      POSTGRES_HOST_AUTH_METHOD: trust
    command:
      - "postgres"
      - "-c"
      - "max_connections=200"
      # --- TH√äM PH·∫¶N N√ÄY CHO OPENMETADATA ---
      - "-c"
      - "shared_preload_libraries=pg_stat_statements"
      - "-c"
      - "pg_stat_statements.track=all"
      - "-c"
      - "pg_stat_statements.max=10000"
      - "-c"
      - "track_activity_query_size=2048"
    ports:
      - "5434:5432"
    volumes:
      - postgres_dest_data:/var/lib/postgresql/data
      - ./postgresql/init_dest.sql:/docker-entrypoint-initdb.d/init.sql
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U admin -d jadc2_db"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - jadc2

  # ============================================================================
  # DATA GENERATOR
  # ============================================================================
  data-generator:
    container_name: dp_data_generator
    build:
      context: ./data_generator
      dockerfile: Dockerfile
    environment:
      POSTGRES_DB: jadc2_db
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: password
      POSTGRES_HOST: postgres-source
      POSTGRES_PORT: 5432
      CONTINUOUS_RUN: "true"
      LOOP_INTERVAL_SECONDS: 300
    depends_on:
      postgres-source:
        condition: service_healthy
    healthcheck:
      interval: 15s
      timeout: 10s
      retries: 10
    networks:
      - jadc2

  # ============================================================================
  # KAFKA (KRaft mode - no Zookeeper needed)
  # ============================================================================
  kafka:
    container_name: dp_kafka
    image: apache/kafka:latest
    ports:
      - "9092:9092"
      - "29092:29092"
    environment:
      # KRaft mode required settings:
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      # define listeners (internal controller + external broker)
      KAFKA_LISTENERS: CONTROLLER://:9093,PLAINTEXT://:9092,PLAINTEXT_HOST://0.0.0.0:29092
      # advertise to clients (outside docker) and internal network
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      # single-node cluster: single controller quorum voter
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka:9093
      # internal topics replication (must be 1 if only one broker)
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      # keep your existing settings if still needed
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
    healthcheck:
      test: nc -z dp_kafka 9092 || exit 1
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 60s
    volumes:
      - kafka_data:/var/lib/kafka/data
    networks:
      - jadc2

  # ============================================================================
  # DEBEZIUM CDC
  # ============================================================================
  debezium:
    container_name: dp_debezium
    image: debezium/connect:2.3
    depends_on:
      kafka:
        condition: service_healthy
      postgres-source:
        condition: service_healthy
    ports:
      - "8083:8083"
    environment:
      BOOTSTRAP_SERVERS: kafka:9092
      GROUP_ID: "1"
      CONFIG_STORAGE_TOPIC: connect_configs
      OFFSET_STORAGE_TOPIC: connect_offsets
      STATUS_STORAGE_TOPIC: connect_statuses
      KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      KEY_CONVERTER_SCHEMAS_ENABLE: "false"
      VALUE_CONVERTER_SCHEMAS_ENABLE: "false"
      OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONFIG_STORAGE_REPLICATION_FACTOR: 1
      STATUS_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_REST_PORT: 8083
      CONNECT_REST_ADVERTISED_HOST_NAME: debezium
      TASKS_MAX: 1
      ENABLE_DEBEZIUM_SCRIPTING: "true"
      CONNECT_LOG4J_ROOT_LOGLEVEL: INFO
    volumes:
      - ./debezium/register-postgres.json:/kafka/config/register-postgres.json:ro
    command: >
      bash -c '
        echo "Waiting for Kafka..."
        cub kafka-ready -b kafka:9092 1 30

        echo "Starting Kafka Connect..."
        /docker-entrypoint.sh start &

        echo "Waiting for Connect to start..."
        until curl -s -o /dev/null -w "%{http_code}" http://localhost:8083/connectors | grep -q 200; do
          sleep 2
        done

        echo "Registering connector..."
        curl -i -X POST -H "Accept:application/json" -H "Content-Type:application/json" http://localhost:8083/connectors/ -d @/kafka/config/register-postgres.json || true

        wait
      '
    healthcheck:
      test: curl -f http://localhost:8083/ || exit 1
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - jadc2

  # ============================================================================
  # KAFKA UI
  # ============================================================================
  kafka-ui:
    container_name: dp_kafka_ui
    image: provectuslabs/kafka-ui:latest
    depends_on:
      kafka:
        condition: service_healthy
    ports:
      - "8086:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:9092
    healthcheck:
      test: curl -f http://localhost:8080 || exit 1
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - jadc2

  # ============================================================================
  # HDFS CLUSTER
  # ============================================================================
  hdfs-namenode:
    container_name: dp_hdfs_namenode
    image: apache/hadoop:3
    hostname: namenode
    # CH·∫†Y B·∫∞NG ROOT ƒê·ªÇ GHI ƒê∆Ø·ª¢C V√ÄO VOLUME
    user: "root"
    # Script: C·∫•p quy·ªÅn 777 cho th∆∞ m·ª•c -> Ki·ªÉm tra ƒë√£ format ch∆∞a -> Format -> Ch·∫°y
    command: ["/bin/bash", "-c", "chmod -R 777 /tmp/hadoop-root/dfs/name; if [ ! -d /tmp/hadoop-root/dfs/name/current ]; then echo 'Formatting NameNode...'; hdfs namenode -format -force -nonInteractive; fi; hdfs namenode"]
    ports:
      - "9870:9870"
      - "9003:9000"
    environment:
      # B·∫Øt bu·ªôc khi ch·∫°y user root
      HDFS_NAMENODE_USER: "root"
      CORE-SITE.XML_fs.defaultFS: "hdfs://namenode:9000"
      HDFS-SITE.XML_dfs.replication: "1"
      HDFS-SITE.XML_dfs.namenode.name.dir: "/tmp/hadoop-root/dfs/name"
    volumes:
      - namenode_data:/tmp/hadoop-root/dfs/name
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9870 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    networks:
      - jadc2

  hdfs-datanode:
    container_name: dp_hdfs_datanode
    image: apache/hadoop:3
    command: ["hdfs", "datanode"]
    # CH·∫†Y B·∫∞NG ROOT
    user: "root"
    environment:
      HDFS_DATANODE_USER: "root"
      CORE-SITE.XML_fs.defaultFS: "hdfs://namenode:9000"
      HDFS-SITE.XML_dfs.replication: "1"
    volumes:
      - datanode_data:/tmp/hadoop-root/dfs/data
    depends_on:
      hdfs-namenode:
        condition: service_healthy
    networks:
      - jadc2

  hdfs-resourcemanager:
    container_name: dp_hdfs_resourcemanager
    image: apache/hadoop:3
    hostname: resourcemanager
    command: ["yarn", "resourcemanager"]
    user: "root"
    ports:
      - "8088:8088"
    environment:
      YARN_RESOURCEMANAGER_USER: "root"
      CORE-SITE.XML_fs.defaultFS: "hdfs://namenode:9000"
      HDFS-SITE.XML_dfs.replication: "1"
      YARN-SITE.XML_yarn.resourcemanager.hostname: "resourcemanager"
    volumes:
      - ./test.sh:/opt/test.sh
    depends_on:
      hdfs-namenode:
        condition: service_healthy
    networks:
      - jadc2

  hdfs-nodemanager:
    container_name: dp_hdfs_nodemanager
    image: apache/hadoop:3
    command: ["yarn", "nodemanager"]
    user: "root"
    environment:
      YARN_NODEMANAGER_USER: "root"
      CORE-SITE.XML_fs.defaultFS: "hdfs://namenode:9000"
      HDFS-SITE.XML_dfs.replication: "1"
      YARN-SITE.XML_yarn.resourcemanager.hostname: "resourcemanager"
    depends_on:
      hdfs-namenode:
        condition: service_healthy
    networks:
      - jadc2   

  # ============================================================================
  # HIVE METASTORE DATABASE (PostgreSQL)
  # ============================================================================
  hms-db:
    container_name: dp_hms_db
    image: postgres:14
    environment:
      POSTGRES_DB: metastore          # ƒê√£ s·ª≠a th√†nh 'metastore'
      POSTGRES_USER: hive
      POSTGRES_PASSWORD: hive
      POSTGRES_INITDB_ARGS: --data-checksums
      POSTGRES_HOST_AUTH_METHOD: trust
    command: ["postgres", "-c", "wal_level=logical"]
    ports:
      - "5435:5432"
    volumes:
      - hms_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U hive -d metastore"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - jadc2

  # ============================================================================
  # HIVE METASTORE SERVICE
  # ============================================================================
  hms:
    container_name: dp_hive_metastore
    build:
      context: ./hive-metastore
      dockerfile: Dockerfile
    hostname: hms
    depends_on:
      hdfs-namenode:
        condition: service_healthy
      hms-db:
        condition: service_healthy
    ports:
      - "9083:9083"
    environment:
      # C√°c bi·∫øn n√†y s·∫Ω ƒë∆∞·ª£c entrypoint.sh s·ª≠ d·ª•ng
      POSTGRES_HOST: hms-db
      POSTGRES_PORT: 5432
      POSTGRES_DB: metastore
      POSTGRES_USER: hive
      POSTGRES_PASSWORD: hive
      HDFS_NAMENODE_HOST: namenode
      HDFS_NAMENODE_PORT: 9000
      METASTORE_PORT: 9083
    # KH√îNG D√ôNG 'command' ·ªû ƒê√ÇY ƒë·ªÉ Docker d√πng entrypoint.sh trong image
    volumes:
      # Mount file c·∫•u h√¨nh v√†o ƒë√∫ng ƒë∆∞·ªùng d·∫´n c·ªßa Image apache/hive
      - ./hive-metastore/conf/hive-site.xml:/opt/hive/conf/hive-site.xml:ro
    healthcheck:
      test: ["CMD-SHELL", "/opt/healthcheck.sh"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - jadc2

  # ============================================================================
  # SPARK CLUSTER
  # ============================================================================
  spark-master:
    container_name: dp_spark_master
    build:
      context: ./spark
      dockerfile: Dockerfile
    hostname: spark-master
    depends_on:
      hdfs-namenode:
        condition: service_healthy
      hms:
        condition: service_healthy
    ports:
      - "7077:7077"
      - "8082:8080"
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=yes
      - SPARK_RPC_AUTHENTICATION_SECRET=jadc2-secret
      # Bind r√µ r√†ng ƒë·ªÉ tr√°nh l·ªói network
      - SPARK_MASTER_HOST=spark-master
    volumes:
      - ./spark/jobs:/opt/spark/jobs:ro
      - ./spark/conf/hive-site.xml:/opt/spark/conf/hive-site.xml:ro
      - ./spark/conf/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf:ro
    # S·ª¨A L·ªñI: Ch·∫°y class Master thay v√¨ tail -f
    command: ["/opt/spark/bin/spark-class", "org.apache.spark.deploy.master.Master"]
    networks:
      - jadc2

  spark-worker:  
    container_name: dp_spark_worker
    build:
      context: ./spark
      dockerfile: Dockerfile
    hostname: spark-worker
    depends_on:
      - spark-master
    ports:
      - "8084:8081"
    environment:
      - SPARK_MODE=worker
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
      - SPARK_RPC_AUTHENTICATION_ENABLED=yes
      - SPARK_RPC_AUTHENTICATION_SECRET=jadc2-secret
    volumes:
      - ./spark/jobs:/opt/spark/jobs:ro
      - ./spark/conf/hive-site.xml:/opt/spark/conf/hive-site.xml:ro
      - ./spark/conf/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf:ro
    # S·ª¨A L·ªñI: Ch·∫°y class Worker v√† tr·ªè v·ªÅ Master
    command: ["/opt/spark/bin/spark-class", "org.apache.spark.deploy.worker.Worker", "spark://spark-master:7077"]
    networks:
      - jadc2

  # ============================================================================
  # SPARK JOBS
  # ============================================================================
  spark-job-ingest:
    container_name: dp_spark_ingest
    build:
      context: ./spark
      dockerfile: Dockerfile
    depends_on:
      spark-master:
        condition: service_started
      kafka:
        condition: service_healthy
      hms:
        condition: service_healthy
    environment:
      - SPARK_RPC_AUTHENTICATION_ENABLED=yes
      - SPARK_RPC_AUTHENTICATION_SECRET=jadc2-secret
    volumes:
      - ./spark/jobs:/opt/spark/jobs:ro
      - ./spark/conf/hive-site.xml:/opt/spark/conf/hive-site.xml:ro
      - ./spark/conf/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf:ro
    command:
      - bash
      - -c
      - |
        echo 'Waiting for Spark Master...'
        while ! nc -z spark-master 7077; do sleep 2; done

        echo 'Waiting for Kafka...'
        while ! nc -z kafka 9092; do sleep 2; done
        
        echo 'Submitting Ingestion Job...'
        /opt/spark/bin/spark-submit \
          --master spark://spark-master:7077 \
          --name 'Job1_Ingest_Kafka_To_HDFS' \
          --deploy-mode client \
          /opt/spark/jobs/job1_ingest.py
    networks:
      - jadc2

  spark-job-process:
    container_name: dp_spark_process
    build:
      context: ./spark
      dockerfile: Dockerfile
    depends_on:
      spark-master:
        condition: service_started
      hms:
        condition: service_healthy
      postgres-dest:
        condition: service_healthy
    environment:
      - SPARK_RPC_AUTHENTICATION_ENABLED=yes
      - SPARK_RPC_AUTHENTICATION_SECRET=jadc2-secret
    volumes:
      - ./spark/jobs:/opt/spark/jobs:ro
      - ./spark/conf/hive-site.xml:/opt/spark/conf/hive-site.xml:ro
      - ./spark/conf/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf:ro
      # üëá TH√äM D√íNG N√ÄY: Mount th∆∞ m·ª•c scripts v√†o container
      - ./spark/scripts:/opt/spark/scripts:ro 
    command:
      - bash
      - -c
      - |
        echo 'Waiting for Spark Master...'
        while ! nc -z spark-master 7077; do sleep 2; done
        
        echo 'Waiting for Postgres Destination...'
        while ! nc -z postgres-dest 5432; do sleep 2; done

        echo 'Checking Data Availability (Silver Layer)...'
        
        # üëá ƒêO·∫†N S·ª¨A: D√πng spark-submit ch·∫°y script python ƒë·ªÉ check path
        # L∆∞u √Ω: ƒê·ªïi path ki·ªÉm tra sang 'silver/regions' v√¨ Job 1 vi·∫øt v√†o silver
        until /opt/spark/bin/spark-submit --master local[*] /opt/spark/scripts/check_hdfs_path.py "hdfs://namenode:9000/data/delta/silver/regions/_delta_log"; do
          echo 'Silver data not found yet. Sleeping 20s...'
          sleep 20
        done
        
        echo 'Data Found! Submitting Processing Job...'
        /opt/spark/bin/spark-submit \
          --master spark://spark-master:7077 \
          --name 'Job2_Process_HDFS_To_Postgres' \
          --deploy-mode client \
          /opt/spark/jobs/job2_process.py
    networks:
      - jadc2
      
#=============================================================================
# VOLUMES
# ============================================================================
volumes:
  # Existing volumes
  postgres_source_data:
  postgres_dest_data:
  kafka_data:
  namenode_data:
  datanode_data:
  hms_data:

# ============================================================================
# NETWORKS
# ============================================================================
networks:
  jadc2:
    name: jadc2
    driver: bridge
