version: "3.8"
services:
  # ============================================================================
  # SOURCE DATABASE (Có cấu hình WAL cho Debezium + Stats cho OpenMetadata)
  # ============================================================================
  postgres-source:
    container_name: dp_postgres_source
    image: postgres:14
    environment:
      POSTGRES_DB: jadc2_db
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: password
      POSTGRES_INITDB_ARGS: --data-checksums
      POSTGRES_HOST_AUTH_METHOD: trust
    command:
      - "postgres"
      - "-c"
      - "wal_level=logical"          # Cần cho Debezium (CDC)
      - "-c"
      - "max_wal_senders=10"
      - "-c"
      - "max_replication_slots=10"
      - "-c"
      - "wal_sender_timeout=0"
      - "-c"
      - "max_connections=200"
      # --- THÊM PHẦN NÀY CHO OPENMETADATA ---
      - "-c"
      - "shared_preload_libraries=pg_stat_statements"
      - "-c"
      - "pg_stat_statements.track=all"
      - "-c"
      - "pg_stat_statements.max=10000"
      - "-c"
      - "track_activity_query_size=2048"
    ports:
      - "5433:5432"
    volumes:
      - postgres_source_data:/var/lib/postgresql/data
      - ./postgresql/init_source.sql:/docker-entrypoint-initdb.d/init.sql
      - ./postgresql/create_publication.sql:/docker-entrypoint-initdb.d/create_publication.sql
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U admin -d jadc2_db"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - jadc2
    
  # ============================================================================
  # DESTINATION DATABASE (Thêm Stats cho OpenMetadata)
  # ============================================================================
  postgres-dest:
    container_name: dp_postgres_dest
    image: postgres:14
    environment:
      POSTGRES_DB: jadc2_db
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: password
      POSTGRES_INITDB_ARGS: --data-checksums
      POSTGRES_HOST_AUTH_METHOD: trust
    command:
      - "postgres"
      - "-c"
      - "max_connections=200"
      # --- THÊM PHẦN NÀY CHO OPENMETADATA ---
      - "-c"
      - "shared_preload_libraries=pg_stat_statements"
      - "-c"
      - "pg_stat_statements.track=all"
      - "-c"
      - "pg_stat_statements.max=10000"
      - "-c"
      - "track_activity_query_size=2048"
    ports:
      - "5434:5432"
    volumes:
      - postgres_dest_data:/var/lib/postgresql/data
      - ./postgresql/init_dest.sql:/docker-entrypoint-initdb.d/init.sql
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U admin -d jadc2_db"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - jadc2

  # ============================================================================
  # DATA GENERATOR
  # ============================================================================
  data-generator:
    container_name: dp_data_generator
    build:
      context: ./data_generator
      dockerfile: Dockerfile
    environment:
      POSTGRES_DB: jadc2_db
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: password
      POSTGRES_HOST: postgres-source
      POSTGRES_PORT: 5432
      CONTINUOUS_RUN: "true"
      LOOP_INTERVAL_SECONDS: 300
    depends_on:
      postgres-source:
        condition: service_healthy
    healthcheck:
      interval: 15s
      timeout: 10s
      retries: 10
    networks:
      - jadc2

  # ============================================================================
  # KAFKA (KRaft mode - no Zookeeper needed)
  # ============================================================================
  kafka:
    container_name: dp_kafka
    image: apache/kafka:latest
    ports:
      - "9092:9092"
      - "29092:29092"
    environment:
      # KRaft mode required settings:
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      # define listeners (internal controller + external broker)
      KAFKA_LISTENERS: CONTROLLER://:9093,PLAINTEXT://:9092,PLAINTEXT_HOST://0.0.0.0:29092
      # advertise to clients (outside docker) and internal network
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      # single-node cluster: single controller quorum voter
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka:9093
      # internal topics replication (must be 1 if only one broker)
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      # keep your existing settings if still needed
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
    healthcheck:
      test: nc -z dp_kafka 9092 || exit 1
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 60s
    volumes:
      - kafka_data:/var/lib/kafka/data
    networks:
      - jadc2

  # ============================================================================
  # DEBEZIUM CDC
  # ============================================================================
  debezium:
    container_name: dp_debezium
    image: debezium/connect:2.3
    depends_on:
      kafka:
        condition: service_healthy
      postgres-source:
        condition: service_healthy
    ports:
      - "8083:8083"
    environment:
      BOOTSTRAP_SERVERS: kafka:9092
      GROUP_ID: "1"
      CONFIG_STORAGE_TOPIC: connect_configs
      OFFSET_STORAGE_TOPIC: connect_offsets
      STATUS_STORAGE_TOPIC: connect_statuses
      KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      KEY_CONVERTER_SCHEMAS_ENABLE: "false"
      VALUE_CONVERTER_SCHEMAS_ENABLE: "false"
      OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONFIG_STORAGE_REPLICATION_FACTOR: 1
      STATUS_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_REST_PORT: 8083
      CONNECT_REST_ADVERTISED_HOST_NAME: debezium
      TASKS_MAX: 1
      ENABLE_DEBEZIUM_SCRIPTING: "true"
      CONNECT_LOG4J_ROOT_LOGLEVEL: INFO
    volumes:
      - ./debezium/register-postgres.json:/kafka/config/register-postgres.json:ro
    command: >
      bash -c '
        echo "Waiting for Kafka..."
        cub kafka-ready -b kafka:9092 1 30

        echo "Starting Kafka Connect..."
        /docker-entrypoint.sh start &

        echo "Waiting for Connect to start..."
        until curl -s -o /dev/null -w "%{http_code}" http://localhost:8083/connectors | grep -q 200; do
          sleep 2
        done

        echo "Registering connector..."
        curl -i -X POST -H "Accept:application/json" -H "Content-Type:application/json" http://localhost:8083/connectors/ -d @/kafka/config/register-postgres.json || true

        wait
      '
    healthcheck:
      test: curl -f http://localhost:8083/ || exit 1
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - jadc2

  # ============================================================================
  # KAFKA UI
  # ============================================================================
  kafka-ui:
    container_name: dp_kafka_ui
    image: provectuslabs/kafka-ui:latest
    depends_on:
      kafka:
        condition: service_healthy
    ports:
      - "8086:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:9092
    healthcheck:
      test: curl -f http://localhost:8080 || exit 1
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - jadc2

  # ============================================================================
  # HDFS CLUSTER
  # ============================================================================
  hdfs-namenode:
    container_name: dp_hdfs_namenode
    image: apache/hadoop:3
    hostname: namenode
    # CHẠY BẰNG ROOT ĐỂ GHI ĐƯỢC VÀO VOLUME
    user: "root"
    # Script: Cấp quyền 777 cho thư mục -> Kiểm tra đã format chưa -> Format -> Chạy
    command: ["/bin/bash", "-c", "chmod -R 777 /tmp/hadoop-root/dfs/name; if [ ! -d /tmp/hadoop-root/dfs/name/current ]; then echo 'Formatting NameNode...'; hdfs namenode -format -force -nonInteractive; fi; hdfs namenode"]
    ports:
      - "9870:9870"
      - "9003:9000"
    environment:
      # Bắt buộc khi chạy user root
      HDFS_NAMENODE_USER: "root"
      CORE-SITE.XML_fs.defaultFS: "hdfs://namenode:9000"
      HDFS-SITE.XML_dfs.replication: "1"
      HDFS-SITE.XML_dfs.namenode.name.dir: "/tmp/hadoop-root/dfs/name"
    volumes:
      - namenode_data:/tmp/hadoop-root/dfs/name
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9870 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    networks:
      - jadc2

  hdfs-datanode:
    container_name: dp_hdfs_datanode
    image: apache/hadoop:3
    command: ["hdfs", "datanode"]
    # CHẠY BẰNG ROOT
    user: "root"
    environment:
      HDFS_DATANODE_USER: "root"
      CORE-SITE.XML_fs.defaultFS: "hdfs://namenode:9000"
      HDFS-SITE.XML_dfs.replication: "1"
    volumes:
      - datanode_data:/tmp/hadoop-root/dfs/data
    depends_on:
      hdfs-namenode:
        condition: service_healthy
    networks:
      - jadc2

  hdfs-resourcemanager:
    container_name: dp_hdfs_resourcemanager
    image: apache/hadoop:3
    hostname: resourcemanager
    command: ["yarn", "resourcemanager"]
    user: "root"
    ports:
      - "8088:8088"
    environment:
      YARN_RESOURCEMANAGER_USER: "root"
      CORE-SITE.XML_fs.defaultFS: "hdfs://namenode:9000"
      HDFS-SITE.XML_dfs.replication: "1"
      YARN-SITE.XML_yarn.resourcemanager.hostname: "resourcemanager"
    volumes:
      - ./test.sh:/opt/test.sh
    depends_on:
      hdfs-namenode:
        condition: service_healthy
    networks:
      - jadc2

  hdfs-nodemanager:
    container_name: dp_hdfs_nodemanager
    image: apache/hadoop:3
    command: ["yarn", "nodemanager"]
    user: "root"
    environment:
      YARN_NODEMANAGER_USER: "root"
      CORE-SITE.XML_fs.defaultFS: "hdfs://namenode:9000"
      HDFS-SITE.XML_dfs.replication: "1"
      YARN-SITE.XML_yarn.resourcemanager.hostname: "resourcemanager"
    depends_on:
      hdfs-namenode:
        condition: service_healthy
    networks:
      - jadc2   

  # ============================================================================
  # HIVE METASTORE DATABASE (PostgreSQL)
  # ============================================================================
  hms-db:
    container_name: dp_hms_db
    image: postgres:14
    environment:
      POSTGRES_DB: metastore          # Đã sửa thành 'metastore'
      POSTGRES_USER: hive
      POSTGRES_PASSWORD: hive
      POSTGRES_INITDB_ARGS: --data-checksums
      POSTGRES_HOST_AUTH_METHOD: trust
    command: ["postgres", "-c", "wal_level=logical"]
    ports:
      - "5435:5432"
    volumes:
      - hms_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U hive -d metastore"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - jadc2

  # ============================================================================
  # HIVE METASTORE SERVICE
  # ============================================================================
  hms:
    container_name: hms
    build:
      context: ./hive-metastore
      dockerfile: Dockerfile
    hostname: hms
    depends_on:
      hdfs-namenode:
        condition: service_healthy
      hms-db:
        condition: service_healthy
    ports:
      - "9083:9083"
    environment:
      # Các biến này sẽ được entrypoint.sh sử dụng
      POSTGRES_HOST: hms-db
      POSTGRES_PORT: 5432
      POSTGRES_DB: metastore
      POSTGRES_USER: hive
      POSTGRES_PASSWORD: hive
      HDFS_NAMENODE_HOST: namenode
      HDFS_NAMENODE_PORT: 9000
      METASTORE_PORT: 9083
    # KHÔNG DÙNG 'command' Ở ĐÂY để Docker dùng entrypoint.sh trong image
    volumes:
      # Mount file cấu hình vào đúng đường dẫn của Image apache/hive
      - ./hive-metastore/conf/hive-site.xml:/opt/hive/conf/hive-site.xml:ro
    healthcheck:
      test: ["CMD-SHELL", "/opt/healthcheck.sh"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - jadc2

  # ============================================================================
  # SPARK CLUSTER
  # ============================================================================
  spark-master:
    container_name: dp_spark_master
    build:
      context: ./spark
      dockerfile: Dockerfile
    hostname: spark-master
    depends_on:
      hdfs-namenode:
        condition: service_healthy
      hms:
        condition: service_healthy
    ports:
      - "7077:7077"
      - "8082:8080"
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=yes
      - SPARK_RPC_AUTHENTICATION_SECRET=jadc2-secret
      # Bind rõ ràng để tránh lỗi network
      - SPARK_MASTER_HOST=spark-master
    volumes:
      - ./spark/jobs:/opt/spark/jobs:ro
      - ./spark/conf/hive-site.xml:/opt/spark/conf/hive-site.xml:ro
      - ./spark/conf/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf:ro
    # SỬA LỖI: Chạy class Master thay vì tail -f
    command: ["/opt/spark/bin/spark-class", "org.apache.spark.deploy.master.Master"]
    networks:
      - jadc2

  spark-worker:  
    container_name: dp_spark_worker
    build:
      context: ./spark
      dockerfile: Dockerfile
    hostname: spark-worker
    depends_on:
      - spark-master
    ports:
      - "8084:8081"
    environment:
      - SPARK_MODE=worker
      - SPARK_WORKER_CORES=4
      - SPARK_WORKER_MEMORY=4g
      - SPARK_RPC_AUTHENTICATION_ENABLED=yes
      - SPARK_RPC_AUTHENTICATION_SECRET=jadc2-secret
    volumes:
      - ./spark/jobs:/opt/spark/jobs:ro
      - ./spark/conf/hive-site.xml:/opt/spark/conf/hive-site.xml:ro
      - ./spark/conf/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf:ro
    # SỬA LỖI: Chạy class Worker và trỏ về Master
    command: ["/opt/spark/bin/spark-class", "org.apache.spark.deploy.worker.Worker", "spark://spark-master:7077"]
    networks:
      - jadc2

  # ============================================================================
  # SPARK JOBS
  # ============================================================================
  spark-job-ingest:
    container_name: dp_spark_ingest
    build:
      context: ./spark
      dockerfile: Dockerfile
    depends_on:
      spark-master:
        condition: service_started
      kafka:
        condition: service_healthy
      hms:
        condition: service_healthy
      openmetadata-server:
        condition: service_started
    environment:
      - SPARK_RPC_AUTHENTICATION_ENABLED=yes
      - SPARK_RPC_AUTHENTICATION_SECRET=jadc2-secret
      - HADOOP_USER_NAME=root
      - OM_SERVER_HOST=openmetadata-server
    volumes:
      - ./spark/scripts:/opt/scripts:ro
      - ./spark/jobs:/opt/spark/jobs:ro
      - ./spark/conf/hive-site.xml:/opt/spark/conf/hive-site.xml:ro
      - ./spark/conf/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf:ro
    command:
      - bash
      - -c
      - |
        source /opt/scripts/get_om_token.sh

        # Kiểm tra xem Token có lấy được không
        if [ -z "$OM_JWT_TOKEN" ]; then
          echo "ERROR: Failed to retrieve OM Token. Exiting."
          exit 1
        fi
        echo "Token retrieved successfully."

        echo 'Waiting for Spark Master...'
        while ! nc -z spark-master 7077; do sleep 2; done

        echo 'Waiting for Kafka...'
        while ! nc -z kafka 9092; do sleep 2; done
        
        echo 'Submitting Ingestion Job...'
        /opt/spark/bin/spark-submit \
          --master spark://spark-master:7077 \
          --name 'Job1_Ingest_Kafka_To_HDFS' \
          --deploy-mode client \
          --conf spark.hadoop.hive.metastore.uris=thrift://hms:9083 \
          --conf spark.cores.max=2 \
          --conf spark.executor.memory=1500m \
          --conf spark.openmetadata.identity.authorization.token="$OM_JWT_TOKEN" \
          /opt/spark/jobs/job1_demo.py
    networks:
      - jadc2

  spark-job-process:
    container_name: dp_spark_process
    build:
      context: ./spark
      dockerfile: Dockerfile
    depends_on:
      spark-master:
        condition: service_started
      hms:
        condition: service_healthy
      postgres-dest:
        condition: service_healthy
      openmetadata-server:
        condition: service_started
    environment:
      - SPARK_RPC_AUTHENTICATION_ENABLED=yes
      - SPARK_RPC_AUTHENTICATION_SECRET=jadc2-secret
      - HADOOP_USER_NAME=root
      - OM_SERVER_HOST=openmetadata-server
    volumes:
      - ./spark/jobs:/opt/spark/jobs:ro
      - ./spark/conf/hive-site.xml:/opt/spark/conf/hive-site.xml:ro
      - ./spark/conf/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf:ro
      - ./spark/scripts:/opt/spark/scripts:ro
    command:
      - bash
      - -c
      - |
        source /opt/scripts/get_om_token.sh

        # Kiểm tra xem Token có lấy được không
        if [ -z "$OM_JWT_TOKEN" ]; then
          echo "ERROR: Failed to retrieve OM Token. Exiting."
          exit 1
        fi
        echo "Token retrieved successfully."
        
        echo 'Waiting for Spark Master...'
        while ! nc -z spark-master 7077; do sleep 2; done
        
        echo 'Waiting for Postgres Destination...'
        while ! nc -z postgres-dest 5432; do sleep 2; done

        echo 'Checking Data Availability (Silver Layer)...'
        
        until /opt/spark/bin/spark-submit --master local[*] /opt/spark/scripts/check_hdfs_path.py "hdfs://namenode:9000/data/delta/silver/regions/_delta_log"; do
          echo 'Silver data not found yet. Sleeping 20s...'
          sleep 20
        done
        
        echo 'Data Found! Submitting Processing Job...'
        /opt/spark/bin/spark-submit \
          --master spark://spark-master:7077 \
          --name 'Job2_Process_HDFS_To_Postgres' \
          --deploy-mode client \
          --conf spark.hadoop.hive.metastore.uris=thrift://hms:9083 \
          --conf spark.cores.max=2 \
          --conf spark.executor.memory=1500m \
          --conf spark.openmetadata.identity.authorization.token="$OM_JWT_TOKEN" \
          /opt/spark/jobs/job2_demo.py
    networks:
      - jadc2

  # ==========================================================================
  # OPENMETADATA - MYSQL DATABASE
  # ==========================================================================
  openmetadata-mysql:
    container_name: openmetadata_mysql
    image: docker.getcollate.io/openmetadata/db:1.10.14
    command: "--sort_buffer_size=10M"
    restart: always
    environment:
      MYSQL_ROOT_PASSWORD: password
    expose:
      - 3306
    ports:
      - "3307:3306"
    volumes:
      - om-db-data:/var/lib/mysql
    networks:
      - jadc2
    healthcheck:
      test: mysql --user=root --password=$$MYSQL_ROOT_PASSWORD --silent --execute "use openmetadata_db"
      interval: 15s
      timeout: 10s
      retries: 10

  # ==========================================================================
  # OPENMETADATA - ELASTICSEARCH
  # ==========================================================================
  openmetadata-elasticsearch:
    container_name: openmetadata_elasticsearch
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.4
    environment:
      - discovery.type=single-node
      - ES_JAVA_OPTS=-Xms1024m -Xmx1024m
      - xpack.security.enabled=false
    networks:
      - jadc2
    ports:
      - "9200:9200"
      - "9300:9300"
    healthcheck:
      test: "curl -s http://localhost:9200/_cluster/health?pretty | grep status | grep -qE 'green|yellow' || exit 1"
      interval: 15s
      timeout: 10s
      retries: 10
    volumes:
      - om-es-data:/usr/share/elasticsearch/data

  # ==========================================================================
  # OPENMETADATA - DATABASE MIGRATION
  # ==========================================================================
  openmetadata-migrate:
    container_name: openmetadata_migrate
    image: docker.getcollate.io/openmetadata/server:1.10.14
    command: "./bootstrap/openmetadata-ops.sh migrate"
    environment:
      OPENMETADATA_CLUSTER_NAME: ${OPENMETADATA_CLUSTER_NAME:-openmetadata}
      SERVER_PORT: ${SERVER_PORT:-8585}
      SERVER_ADMIN_PORT: ${SERVER_ADMIN_PORT:-8586}
      LOG_LEVEL: ${LOG_LEVEL:-INFO}
      MIGRATION_LIMIT_PARAM: ${MIGRATION_LIMIT_PARAM:-1200}
      
      # Authentication
      AUTHORIZER_CLASS_NAME: ${AUTHORIZER_CLASS_NAME:-org.openmetadata.service.security.DefaultAuthorizer}
      AUTHORIZER_REQUEST_FILTER: ${AUTHORIZER_REQUEST_FILTER:-org.openmetadata.service.security.JwtFilter}
      AUTHORIZER_ADMIN_PRINCIPALS: ${AUTHORIZER_ADMIN_PRINCIPALS:-[admin]}
      AUTHORIZER_INGESTION_PRINCIPALS: ${AUTHORIZER_INGESTION_PRINCIPALS:-[ingestion-bot]}
      AUTHENTICATION_PROVIDER: ${AUTHENTICATION_PROVIDER:-basic}
      AUTHENTICATION_PUBLIC_KEYS: ${AUTHENTICATION_PUBLIC_KEYS:-[http://localhost:8585/api/v1/system/config/jwks]}
      AUTHENTICATION_ENABLE_SELF_SIGNUP: ${AUTHENTICATION_ENABLE_SELF_SIGNUP:-true}
      
      # JWT
      RSA_PUBLIC_KEY_FILE_PATH: ${RSA_PUBLIC_KEY_FILE_PATH:-"./conf/public_key.der"}
      RSA_PRIVATE_KEY_FILE_PATH: ${RSA_PRIVATE_KEY_FILE_PATH:-"./conf/private_key.der"}
      JWT_ISSUER: ${JWT_ISSUER:-"open-metadata.org"}
      JWT_KEY_ID: ${JWT_KEY_ID:-"Gb389a-9f76-gdjs-a92j-0242bk94356"}
      
      # Pipeline Service
      PIPELINE_SERVICE_CLIENT_ENDPOINT: ${PIPELINE_SERVICE_CLIENT_ENDPOINT:-http://openmetadata-ingestion:8080}
      SERVER_HOST_API_URL: ${SERVER_HOST_API_URL:-http://openmetadata-server:8585/api}
      
      # Database
      DB_DRIVER_CLASS: ${DB_DRIVER_CLASS:-com.mysql.cj.jdbc.Driver}
      DB_SCHEME: ${DB_SCHEME:-mysql}
      DB_PARAMS: ${DB_PARAMS:-allowPublicKeyRetrieval=true&useSSL=false&serverTimezone=UTC}
      DB_USER: ${DB_USER:-openmetadata_user}
      DB_USER_PASSWORD: ${DB_USER_PASSWORD:-openmetadata_password}
      DB_HOST: ${DB_HOST:-openmetadata-mysql}
      DB_PORT: ${DB_PORT:-3306}
      OM_DATABASE: ${OM_DATABASE:-openmetadata_db}
      
      # Elasticsearch
      ELASTICSEARCH_HOST: ${ELASTICSEARCH_HOST:-openmetadata-elasticsearch}
      ELASTICSEARCH_PORT: ${ELASTICSEARCH_PORT:-9200}
      ELASTICSEARCH_SCHEME: ${ELASTICSEARCH_SCHEME:-http}
      SEARCH_TYPE: ${SEARCH_TYPE:-"elasticsearch"}
      
      # Airflow
      AIRFLOW_USERNAME: ${AIRFLOW_USERNAME:-admin}
      AIRFLOW_PASSWORD: ${AIRFLOW_PASSWORD:-admin}
      FERNET_KEY: ${FERNET_KEY:-jJ/9sz0g0OHxsfxOoSfdFdmk3ysNmPRnH3TUAbz3IHA=}
      
      # Secrets Manager
      SECRET_MANAGER: ${SECRET_MANAGER:-db}
      
      # Heap Options
      OPENMETADATA_HEAP_OPTS: ${OPENMETADATA_HEAP_OPTS:--Xmx1G -Xms1G}
    
    expose:
      - 8585
      - 8586
    ports:
      - "8585:8585"
      - "8586:8586"
    depends_on:
      openmetadata-elasticsearch:
        condition: service_healthy
      openmetadata-mysql:
        condition: service_healthy
      openmetadata-migrate:
        condition: service_completed_successfully
    networks:
      - jadc2
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:8586/healthcheck"]

  # ==========================================================================
  # OPENMETADATA - INGESTION (AIRFLOW)
  # ==========================================================================
  openmetadata-ingestion:
    container_name: openmetadata_ingestion
    image: docker.getcollate.io/openmetadata/ingestion:1.10.14
    depends_on:
      openmetadata-elasticsearch:
        condition: service_started
      openmetadata-mysql:
        condition: service_healthy
      openmetadata-server:
        condition: service_started
    environment:
      AIRFLOW__API__AUTH_BACKENDS: "airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session"
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__OPENMETADATA_AIRFLOW_APIS__DAG_GENERATED_CONFIGS: "/opt/airflow/dag_generated_configs"
      DB_HOST: ${AIRFLOW_DB_HOST:-openmetadata-mysql}
      DB_PORT: ${AIRFLOW_DB_PORT:-3306}
      AIRFLOW_DB: ${AIRFLOW_DB:-airflow_db}
      DB_SCHEME: ${AIRFLOW_DB_SCHEME:-mysql+mysqldb}
      DB_USER: ${AIRFLOW_DB_USER:-airflow_user}
      DB_PASSWORD: ${AIRFLOW_DB_PASSWORD:-airflow_pass}
      HEAP_OPTS: ${OPENMETADATA_HEAP_OPTS:--Xmx1G -Xms1G}
    entrypoint: /bin/bash
    command:
      - "/opt/airflow/ingestion_dependency.sh"
    expose:
      - 8080
    ports:
      - "8081:8080"
    networks:
      - jadc2
    volumes:
      - ingestion-volume-dag-airflow:/opt/airflow/dag_generated_configs
      - ingestion-volume-dags:/opt/airflow/dags
      - ingestion-volume-tmp:/tmp
    depends_on:
      openmetadata-elasticsearch:
        condition: service_healthy
      openmetadata-mysql:
        condition: service_healthy
    networks:
      - jadc2

  # ==========================================================================
  # OPENMETADATA - SERVER
  # ==========================================================================
  openmetadata-server:
    container_name: openmetadata_server
    restart: always
    image: docker.getcollate.io/openmetadata/server:1.10.14
    environment:
      OPENMETADATA_CLUSTER_NAME: ${OPENMETADATA_CLUSTER_NAME:-openmetadata}
      SERVER_PORT: ${SERVER_PORT:-8585}
      SERVER_ADMIN_PORT: ${SERVER_ADMIN_PORT:-8586}
      LOG_LEVEL: ${LOG_LEVEL:-INFO}
      
      # Authentication
      AUTHORIZER_CLASS_NAME: ${AUTHORIZER_CLASS_NAME:-org.openmetadata.service.security.DefaultAuthorizer}
      AUTHORIZER_REQUEST_FILTER: ${AUTHORIZER_REQUEST_FILTER:-org.openmetadata.service.security.JwtFilter}
      AUTHORIZER_ADMIN_PRINCIPALS: ${AUTHORIZER_ADMIN_PRINCIPALS:-[admin]}
      AUTHORIZER_INGESTION_PRINCIPALS: ${AUTHORIZER_INGESTION_PRINCIPALS:-[ingestion-bot]}
      AUTHENTICATION_PROVIDER: ${AUTHENTICATION_PROVIDER:-basic}
      AUTHENTICATION_PUBLIC_KEYS: ${AUTHENTICATION_PUBLIC_KEYS:-[http://localhost:8585/api/v1/system/config/jwks]}
      AUTHENTICATION_ENABLE_SELF_SIGNUP: ${AUTHENTICATION_ENABLE_SELF_SIGNUP:-true}
      
      # JWT
      RSA_PUBLIC_KEY_FILE_PATH: ${RSA_PUBLIC_KEY_FILE_PATH:-"./conf/public_key.der"}
      RSA_PRIVATE_KEY_FILE_PATH: ${RSA_PRIVATE_KEY_FILE_PATH:-"./conf/private_key.der"}
      JWT_ISSUER: ${JWT_ISSUER:-"open-metadata.org"}
      JWT_KEY_ID: ${JWT_KEY_ID:-"Gb389a-9f76-gdjs-a92j-0242bk94356"}
      
      # Pipeline Service
      PIPELINE_SERVICE_CLIENT_ENDPOINT: ${PIPELINE_SERVICE_CLIENT_ENDPOINT:-http://openmetadata-ingestion:8080}
      PIPELINE_SERVICE_CLIENT_ENABLED: ${PIPELINE_SERVICE_CLIENT_ENABLED:-true}
      SERVER_HOST_API_URL: ${SERVER_HOST_API_URL:-http://openmetadata-server:8585/api}
      
      # Database
      DB_DRIVER_CLASS: ${DB_DRIVER_CLASS:-com.mysql.cj.jdbc.Driver}
      DB_SCHEME: ${DB_SCHEME:-mysql}
      DB_PARAMS: ${DB_PARAMS:-allowPublicKeyRetrieval=true&useSSL=false&serverTimezone=UTC}
      DB_USER: ${DB_USER:-openmetadata_user}
      DB_USER_PASSWORD: ${DB_USER_PASSWORD:-openmetadata_password}
      DB_HOST: ${DB_HOST:-openmetadata-mysql}
      DB_PORT: ${DB_PORT:-3306}
      OM_DATABASE: ${OM_DATABASE:-openmetadata_db}
      
      # Elasticsearch
      ELASTICSEARCH_HOST: ${ELASTICSEARCH_HOST:-openmetadata-elasticsearch}
      ELASTICSEARCH_PORT: ${ELASTICSEARCH_PORT:-9200}
      ELASTICSEARCH_SCHEME: ${ELASTICSEARCH_SCHEME:-http}
      SEARCH_TYPE: ${SEARCH_TYPE:-"elasticsearch"}
      
      # Airflow
      AIRFLOW_USERNAME: ${AIRFLOW_USERNAME:-admin}
      AIRFLOW_PASSWORD: ${AIRFLOW_PASSWORD:-admin}
      FERNET_KEY: ${FERNET_KEY:-jJ/9sz0g0OHxsfxOoSfdFdmk3ysNmPRnH3TUAbz3IHA=}
      
      #secretsManagerConfiguration
      SECRET_MANAGER: ${SECRET_MANAGER:-db}
      #parameters:
      OM_SM_REGION: ${OM_SM_REGION:-""}
      OM_SM_ACCESS_KEY_ID: ${OM_SM_ACCESS_KEY_ID:-""}
      OM_SM_ACCESS_KEY: ${OM_SM_ACCESS_KEY:-""}
      
      #email configuration:
      OM_EMAIL_ENTITY: ${OM_EMAIL_ENTITY:-"OpenMetadata"}
      OM_SUPPORT_URL: ${OM_SUPPORT_URL:-"https://slack.open-metadata.org"}
      AUTHORIZER_ENABLE_SMTP : ${AUTHORIZER_ENABLE_SMTP:-false}
      OPENMETADATA_SERVER_URL: ${OPENMETADATA_SERVER_URL:-""}
      OPENMETADATA_SMTP_SENDER_MAIL: ${OPENMETADATA_SMTP_SENDER_MAIL:-""}
      SMTP_SERVER_ENDPOINT: ${SMTP_SERVER_ENDPOINT:-""}
      SMTP_SERVER_PORT: ${SMTP_SERVER_PORT:-""}
      SMTP_SERVER_USERNAME: ${SMTP_SERVER_USERNAME:-""}
      SMTP_SERVER_PWD: ${SMTP_SERVER_PWD:-""}
      SMTP_SERVER_STRATEGY: ${SMTP_SERVER_STRATEGY:-"SMTP_TLS"}

      # Heap OPTS Configurations
      OPENMETADATA_HEAP_OPTS: ${OPENMETADATA_HEAP_OPTS:--Xmx1G -Xms1G}
      # Mask passwords values in UI
      MASK_PASSWORDS_API: ${MASK_PASSWORDS_API:-false}
      
      #OpenMetadata Web Configuration
      WEB_CONF_URI_PATH: ${WEB_CONF_URI_PATH:-"/api"}
      #HSTS
      WEB_CONF_HSTS_ENABLED: ${WEB_CONF_HSTS_ENABLED:-false}
      WEB_CONF_HSTS_MAX_AGE: ${WEB_CONF_HSTS_MAX_AGE:-"365 days"}
      WEB_CONF_HSTS_INCLUDE_SUBDOMAINS: ${WEB_CONF_HSTS_INCLUDE_SUBDOMAINS:-"true"}
      WEB_CONF_HSTS_PRELOAD: ${WEB_CONF_HSTS_PRELOAD:-"true"}
      #Frame Options
      WEB_CONF_FRAME_OPTION_ENABLED: ${WEB_CONF_FRAME_OPTION_ENABLED:-false}
      WEB_CONF_FRAME_OPTION: ${WEB_CONF_FRAME_OPTION:-"SAMEORIGIN"}
      WEB_CONF_FRAME_ORIGIN: ${WEB_CONF_FRAME_ORIGIN:-""}
      #Content Type
      WEB_CONF_CONTENT_TYPE_OPTIONS_ENABLED: ${WEB_CONF_CONTENT_TYPE_OPTIONS_ENABLED:-false}
      #XSS-Protection  
      WEB_CONF_XSS_PROTECTION_ENABLED: ${WEB_CONF_XSS_PROTECTION_ENABLED:-false}
      WEB_CONF_XSS_PROTECTION_ON: ${WEB_CONF_XSS_PROTECTION_ON:-true}
      WEB_CONF_XSS_PROTECTION_BLOCK: ${WEB_CONF_XSS_PROTECTION_BLOCK:-true}
      #CSP    
      WEB_CONF_XSS_CSP_ENABLED: ${WEB_CONF_XSS_CSP_ENABLED:-false}
      WEB_CONF_XSS_CSP_POLICY: ${WEB_CONF_XSS_CSP_POLICY:-"default-src 'self'"}
      WEB_CONF_XSS_CSP_REPORT_ONLY_POLICY: ${WEB_CONF_XSS_CSP_REPORT_ONLY_POLICY:-""}
      #Cache
      WEB_CONF_CACHE_CONTROL: ${WEB_CONF_CACHE_CONTROL:-""}
      WEB_CONF_PRAGMA: ${WEB_CONF_PRAGMA:-""}
    
    expose:
      - 8585
      - 8586
    ports:
      - "8585:8585"
      - "8586:8586"
    depends_on:
      openmetadata-elasticsearch:
        condition: service_healthy
      openmetadata-mysql:
        condition: service_healthy
      openmetadata-migrate:
        condition: service_completed_successfully
    networks:
      - jadc2
    healthcheck:
      test: [ "CMD", "wget", "-q", "--spider",  "http://localhost:8586/healthcheck" ]
      
#=============================================================================
# VOLUMES
# ============================================================================
volumes:
  # Existing volumes
  postgres_source_data:
  postgres_dest_data:
  kafka_data:
  namenode_data:
  datanode_data:
  hms_data:

  # OpenMetadata volumes
  ingestion-volume-dag-airflow:
  ingestion-volume-dags:
  ingestion-volume-tmp:
  om-es-data:
  om-db-data:

# ============================================================================
# NETWORKS
# ============================================================================
networks:
  jadc2:
    name: jadc2
    driver: bridge
