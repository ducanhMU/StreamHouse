# spark/conf/spark-defaults.conf
# Global Spark configuration with Delta Lake, OpenMetadata, and JADC2 optimizations

# ==========================================
# APPLICATION CONFIGURATION
# ==========================================
spark.app.name                      JADC2-Pipeline
spark.submit.deployMode             client

# ==========================================
# DELTA LAKE CONFIGURATION
# ==========================================
# Enable Delta Lake extensions and catalog
spark.sql.extensions                io.delta.sql.DeltaSparkSessionExtension
spark.sql.catalog.spark_catalog     org.apache.spark.sql.delta.catalog.DeltaCatalog

# Delta Lake optimizations
spark.databricks.delta.retentionDurationCheck.enabled   false
spark.databricks.delta.schema.autoMerge.enabled         true
spark.databricks.delta.properties.defaults.autoOptimize.optimizeWrite   true
spark.databricks.delta.properties.defaults.autoOptimize.autoCompact     true

# ==========================================
# HIVE METASTORE CONFIGURATION
# ==========================================
# Connect to external Hive Metastore
spark.sql.hive.metastore.version                2.3.9
spark.sql.hive.metastore.jars                   builtin
spark.sql.catalogImplementation                 hive
spark.hadoop.hive.metastore.uris                thrift://hive-metastore:9083

# ==========================================
# HDFS CONFIGURATION
# ==========================================
# Default filesystem
spark.hadoop.fs.defaultFS                       hdfs://namenode:9000

# HDFS optimization
spark.hadoop.dfs.client.use.datanode.hostname   true
spark.hadoop.dfs.client.read.shortcircuit       false

# ==========================================
# OPENMETADATA LINEAGE CONFIGURATION
# ==========================================
# Enable OpenMetadata Spark Agent for automatic lineage tracking
spark.extraListeners                            org.openmetadata.spark.agent.OpenMetadataSparkListener

# OpenMetadata Server Configuration
spark.openmetadata.transport.hostPort           http://om-server:8585
spark.openmetadata.transport.type               openmetadata
spark.openmetadata.transport.timeout            30
spark.openmetadata.transport.trustStore.path    
spark.openmetadata.transport.trustStore.type    JKS

# Authentication (JWT Token - set via environment variable or here)
# spark.openmetadata.transport.headers.Authentication    Bearer <JWT_TOKEN>
# Note: Token should be set via environment variable OM_JWT_TOKEN in docker-compose

# Pipeline Service Configuration
spark.openmetadata.pipelineServiceName          spark_pipeline_service
spark.openmetadata.appName                      ${spark.app.name}

# Lineage Configuration
spark.openmetadata.lineage.timeout              10000
spark.openmetadata.lineage.sendInterval         30

# ==========================================
# KAFKA CONFIGURATION (for Streaming Jobs)
# ==========================================
# Kafka bootstrap servers
spark.kafka.bootstrap.servers                   kafka:9092

# Kafka consumer settings
spark.streaming.kafka.consumer.poll.ms          512
spark.streaming.kafka.maxRatePerPartition       1000

# ==========================================
# MEMORY CONFIGURATION
# ==========================================
# Driver memory
spark.driver.memory                             2g
spark.driver.memoryOverhead                     512m
spark.driver.maxResultSize                      1g

# Executor memory
spark.executor.memory                           2g
spark.executor.memoryOverhead                   512m

# Memory fractions
spark.memory.fraction                           0.8
spark.memory.storageFraction                    0.3

# ==========================================
# EXECUTOR CONFIGURATION
# ==========================================
# Executor settings
spark.executor.instances                        2
spark.executor.cores                            2
spark.default.parallelism                       8
spark.sql.shuffle.partitions                    8

# Dynamic allocation (disable for stability in Docker)
spark.dynamicAllocation.enabled                 false

# ==========================================
# PERFORMANCE TUNING
# ==========================================
# Shuffle configuration
spark.shuffle.service.enabled                   false
spark.shuffle.compress                          true
spark.shuffle.spill.compress                    true
spark.io.compression.codec                      snappy

# Serialization
spark.serializer                                org.apache.spark.serializer.KryoSerializer
spark.kryo.registrationRequired                 false
spark.kryoserializer.buffer.max                 512m

# SQL optimizations
spark.sql.adaptive.enabled                      true
spark.sql.adaptive.coalescePartitions.enabled   true
spark.sql.adaptive.skewJoin.enabled             true
spark.sql.autoBroadcastJoinThreshold            10485760

# Broadcast timeout
spark.sql.broadcastTimeout                      300

# ==========================================
# CHECKPOINT CONFIGURATION
# ==========================================
# Streaming checkpoint location
spark.sql.streaming.checkpointLocation          hdfs://namenode:9000/checkpoints

# ==========================================
# NETWORK CONFIGURATION
# ==========================================
# Network timeouts
spark.network.timeout                           300s
spark.executor.heartbeatInterval                20s
spark.rpc.message.maxSize                       256

# ==========================================
# LOGGING CONFIGURATION
# ==========================================
# Log level (overridden by code typically)
spark.eventLog.enabled                          true
spark.eventLog.dir                              hdfs://namenode:9000/spark-logs
spark.history.fs.logDirectory                   hdfs://namenode:9000/spark-logs

# UI settings
spark.ui.enabled                                true
spark.ui.port                                   4040
spark.ui.retainedJobs                           100
spark.ui.retainedStages                         100

# ==========================================
# SECURITY CONFIGURATION (Development)
# ==========================================
# Authentication disabled for development
spark.authenticate                              false
spark.authenticate.secret                       

# Encryption disabled for development
spark.network.crypto.enabled                    false
spark.io.encryption.enabled                     false

# ==========================================
# JADC2-SPECIFIC CONFIGURATIONS
# ==========================================
# Custom app configurations for JADC2 pipeline
spark.jadc2.source.postgres.url                 jdbc:postgresql://postgres-source:5432/jadc2_db
spark.jadc2.source.postgres.user                admin
spark.jadc2.source.postgres.password            password

spark.jadc2.dest.postgres.url                   jdbc:postgresql://postgres-dest:5432/jadc2_db
spark.jadc2.dest.postgres.user                  admin
spark.jadc2.dest.postgres.password              password

spark.jadc2.hdfs.base.path                      hdfs://namenode:9000/data/delta
spark.jadc2.checkpoint.base.path                hdfs://namenode:9000/checkpoints

# Processing intervals
spark.jadc2.streaming.trigger.interval          60 seconds
spark.jadc2.batch.trigger.interval              5 minutes

# ==========================================
# POSTGRES JDBC CONFIGURATION
# ==========================================
# JDBC connection pooling
spark.jdbc.numPartitions                        4
spark.jdbc.fetchSize                            1000
spark.jdbc.batchSize                            1000

# ==========================================
# CLASSPATH AND JARS
# ==========================================
# Extra JARs (loaded automatically from jars_extra directory)
spark.jars                                      /opt/spark/jars_extra/*.jar

# Driver and executor classpaths
spark.driver.extraClassPath                     /opt/spark/jars_extra/*
spark.executor.extraClassPath                   /opt/spark/jars_extra/*

# ==========================================
# PYTHON CONFIGURATION
# ==========================================
spark.pyspark.driver.python                     python3
spark.pyspark.python                            python3

# ==========================================
# TIMEZONE CONFIGURATION
# ==========================================
# Use UTC for consistency
spark.sql.session.timeZone                      UTC

# ==========================================
# ADVANCED SETTINGS
# ==========================================
# Speculation (disabled for deterministic processing)
spark.speculation                               false

# Task failures
spark.task.maxFailures                          4

# Stage retry
spark.stage.maxConsecutiveAttempts              4

# Blacklisting (disabled in development)
spark.blacklist.enabled                         false